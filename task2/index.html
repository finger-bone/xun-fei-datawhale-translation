<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://finger-bone.github.io/rl-crashcourse/task2/">
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Attention - Reinforcement Learning Crashcourse</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/color-brewer.min.css">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Reinforcement Learning Crashcourse</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../task1/" class="nav-link">RNN</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Attention</a>
                            </li>
                            <li class="navitem">
                                <a href="../task3/" class="nav-link">Transformer</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../task1/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../task3/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#the-night-before-the-transformer" class="nav-link">The Night Before the Transformer</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#a-recapitulation-of-the-previous-model" class="nav-link">A Recapitulation of the Previous Model</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#linear-attention-mechanism" class="nav-link">Linear Attention Mechanism</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#dot-product-attention-mechanism" class="nav-link">Dot-product Attention Mechanism</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#multi-head-attention" class="nav-link">Multi-Head Attention</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#other-improvements" class="nav-link">Other Improvements</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="the-night-before-the-transformer">The Night Before the Transformer</h1>
<h2 id="introduction">Introduction</h2>
<p>The second task aims to improve the performance of the original encoder-rnn-decoder model without introducing the transformer architecture, albeit it has been proven, currently, the best architecture for machine translation tasks.</p>
<h2 id="a-recapitulation-of-the-previous-model">A Recapitulation of the Previous Model</h2>
<p>In task one, there was a encoder-decoder model with rnn networks. However, it behaved poorly due to, mainly, the context compression. The encoder compresses the context into a fixed-length vector, which is not enough to store all the information. It may work for short sentences, but it fails for longer ones.</p>
<p>However, that model got the greater part of the job done. The model was trained on a small dataset, which is not enough to learn the complex patterns of the language. Now, we just need to improve the encoder and decoders.</p>
<h2 id="linear-attention-mechanism">Linear Attention Mechanism</h2>
<p>Attention mechanism, to put simply, is to weight the importance of different parts of the input sequence. It is a mechanism that allows the model to focus on different parts of the input sequence.</p>
<p>An obvious way to improve the encoder-decoder model is to add an extra attention layer in between the encoder and decoder. The attention layer will help the decoder to focus on different parts of the input sequence.</p>
<p>An example is as below,</p>
<pre><code class="language-python">class Attention(nn.Module):
    def __init__(self, hidden_dim, d):
        super(Attention, self).__init__()
        self.w = nn.Linear(hidden_dim * 2, d)
        self.v = nn.Linear(hidden_dim, 1, bias=False)
        self.activation = nn.Tanh()

    def forward(self, x, h):
        h = h.permute(1, 0, 2)
        # h is [batch, num_layers == 1, hidden_dim]
        # x is [batch, len, hidden_dim]
        h = h.expand(-1, x.size(1), -1)
        # [batch, len, hidden_dim * 2] -&gt; [batch, len, d]
        w = self.w(th.cat((x, h), dim=-1))
        # [batch, len, d] -&gt; [batch, len, 1] -&gt; [batch, len]
        attn = self.v(w)
        attn = attn.squeeze(-1)
        return th.softmax(attn, dim=-1)
</code></pre>
<p>In the code above, the <code>attn</code> assumes the role of the attention, which is a tensor of shape <script type="math/tex">(batch, len)</script>, where <script type="math/tex">len</script> is the length of the input sequence. So, each element of the tensor refers to the importance of the corresponding part of the input sequence.</p>
<p>Using that, we can pay attention to the output vector of the encoder so that we can focus on different parts of the input sequence.</p>
<pre><code class="language-python">class Decoder(nn.Module):

    def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -&gt; None:
        super().__init__()
        # -&gt; [batch, len]
        self.attn = Attention(hidden_dim, hidden_dim)
        # [batch, len == 1] -&gt; [batch, len == 1, embed_dim]
        self.embed = nn.Embedding(zh_vocab_size, embed_dim)
        # [len == 1, batch, embed_dim + hidden_dim] -&gt; [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim]
        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim)
        # [batch, hidden_dim * 2 + embed_dim] -&gt; [batch, zh_vocab_size]
        self.fc = nn.Linear(hidden_dim * 2 + embed_dim, zh_vocab_size)
        self.dropout = nn.Dropout(drop_out_rate)
        self.activation = nn.Tanh()

    def forward(self, x, h, enc_out):
        # enc_out: [batch, len, hidden_dim]
        # x is [batch, len == 1]
        # h is [n_layers == 1, batch, hidden_dim]

        attn = self.attn(enc_out, h)
        # [batch, 1, hidden_dim] = [batch, 1, len] * [batch, len, hidden_dim]
        v = th.bmm(attn.unsqueeze(1), enc_out)
        # v: [batch, 1, hidden_dim]

        x = self.embed(x)
        # x: [batch, len == 1, embed_dim]
        x = self.dropout(x)
        rx = th.cat((v, x), dim=-1)
        rx = self.activation(rx)
        # rx: [batch, len == 1, embed_dim + hidden_dim]
        rx = rx.permute(1, 0, 2)
        out_x, h = self.gru(rx, h)
        out_x = out_x.permute(1, 0, 2)
        # out_x: [batch, len == 1, hidden_dim]
        out_x = out_x.squeeze(1)
        v = v.squeeze(1)
        fc_in = th.cat((out_x, v, x.squeeze(1)), dim=-1)

        out_x = self.fc(fc_in)
        return out_x, h
</code></pre>
<p>The result is still horrible. Yet, compared to the previous model, the model can get the length of the sentence almost correct, and it shows some sign of understanding the sentence. After fully training on the provided dataset for one epoch, a much, much better score is achieved.</p>
<p>Nevertheless, the model is still scarcely usable.</p>
<h2 id="dot-product-attention-mechanism">Dot-product Attention Mechanism</h2>
<h3 id="query-key-and-value">Query, Key, and Value</h3>
<p>Dot-product attention is a mechanism that allows the model to focus on different parts of the input sequence. It takes three inputs, query, key, and value, and returns a weighted sum of the values. The query, key, and value are linear transformations of the input sequence.</p>
<p>There are three important concepts in the attention mechanism: query, key, and value.</p>
<ul>
<li>
<p>Query, the subjective attention vector. It is used to signify how subjectively important part of the value is to the current part of the sequence. By subjectively, it means that the model decides how important the part of the value is to the current part of the sequence, based on its previous knowledge.</p>
</li>
<li>
<p>Key, the objective attention vector. It is used to signify how objectively important part of the value is to the current part of the sequence. By objectively, it means that part of the value is inherently more important to the current part of the sequence. For example, link verbs are less important than nouns.</p>
</li>
<li>
<p>Value, the content vector. It is the part of the sequence that the model should focus on.</p>
</li>
</ul>
<p>When the query and key derive from the same input, it is called self attention. When the query and key derive from different inputs, it is called cross attention.</p>
<h3 id="dot-product-attention-math">Dot-product Attention Math</h3>
<p>In the attention layer, firstly, a tensor of shape <script type="math/tex">(len, embed)</script> will be transformed into three tensors with the shape <script type="math/tex">(len, d)</script>, where <script type="math/tex">d</script> is the dimension, can be arbitrary, usually equal to the dimension of the embedding vector, so that the input and output have the same shape.</p>
<p>Then the math is as follows,</p>
<p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V
</script>
</p>
<p>where <script type="math/tex">d</script> is the dimension of the key vector.</p>
<p>Or for clarity, using the abstract indices,</p>
<p>
<script type="math/tex; mode=display">
\text{Attention}(Q_i^j, K_i^j, V_i^j) = \text{softmax}(\frac{Q_i^jK_j^k}{\sqrt{d}})V_k^l
</script>
</p>
<p>Noticing that query and key are actually fungible, our previous definition of query and key is just for the sake of explanation. How it really works is still shrouded in mystery.</p>
<p>The divisor <script type="math/tex">\sqrt{d}</script> is used to prevent the dot product from being too large, which may result in the soft-max function returning 0 or 1. It is purely a practical trick.</p>
<h3 id="implementation">Implementation</h3>
<p>A simple implementation of the dor-product attention mechanism is as follows,</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed, d):
        super(SelfAttention, self).__init__()
        self.Q = nn.Linear(embed, d)
        self.K = nn.Linear(embed, d)
        self.V = nn.Linear(embed, d)
        self.d = d

    def forward(self, x):
        # x is [batch, len, embed]
        # Q, K, V are [batch, len, d]
        Q = self.Q(x)
        K = self.K(x)
        V = self.V(x)

        # Q, K, V are [batch, len, d]
        # QK^T is [batch, len, len]
        # QK^T / sqrt(d) is [batch, len, len]
        # softmax(QK^T / sqrt(d)) is [batch, len, len]
        # softmax(QK^T / sqrt(d))V is [batch, len, d]
        attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5)
        attn = torch.softmax(attn, dim=-1)
        out = torch.matmul(attn, V)
        return out
</code></pre>
<h2 id="multi-head-attention">Multi-Head Attention</h2>
<p>The attention mechanism can be improved by using multiple heads. The multi-head self attention mechanism is a mechanism that allows the model to focus on different parts of the input sequence.</p>
<p>To put it more simply, the multi-head mechanism is to use multiple self attention layers in parallel, after splitting the embedding dimension into small chunks, and then concatenate the results. The multi-head mechanism can help the model to focus on different parts of the input sequence.</p>
<p>The math is as follows,</p>
<p>
<script type="math/tex; mode=display">
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{Head}_1, \text{Head}_2, \ldots, \text{Head}_n)W^O
</script>
</p>
<p>where <script type="math/tex">\text{Head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</script> is a sub-attention, and <script type="math/tex">W^O</script> is a linear transformation.</p>
<p>An simple implementation is as follows, which is a cross attention because the query and key are different.</p>
<pre><code class="language-python">class MultiHeadAttn(nn.Module):

    def __init__(
        self, 
        dim: int, 
        heads: int, 
        dropout: float
    ):
        super(MultiHeadAttn, self).__init__()

        self.heads = heads
        self.dim = dim
        self.head_dim = dim // heads

        assert self.head_dim * heads == dim, &quot;dim must be divisible by heads&quot;

        self.q = nn.Linear(dim, dim)
        self.k = nn.Linear(dim, dim)
        self.v = nn.Linear(dim, dim)

        self.fc_out = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: Tensor, y: Tensor, mask: Tensor | None=None) -&gt; Tensor:
        batch_size = x.shape[0]

        # Linear projections
        Q = self.q(y)
        K = self.k(x)
        V = self.v(x)

        # Reshape for multi-head attention
        Q = Q.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)

        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)  # Adjust mask shape for broadcasting
            energy = energy.masked_fill(mask == 0, float('-inf'))

        attention = torch.softmax(energy, dim=-1)
        attention = self.dropout(attention)

        out = torch.matmul(attention, V)

        # Concatenate heads
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)

        # Final linear layer
        out = self.fc_out(out)

        return out
</code></pre>
<p>However, this part will not use the multi-head self attention mechanism. We will use them in the transformer architecture.</p>
<h2 id="other-improvements">Other Improvements</h2>
<p>Without tweaking the model too much, here are some other improvements that can be made to the model.</p>
<ul>
<li>Use LSTM instead of GRU. LSTM is more powerful than GRU, and it can learn more complex patterns in the data.</li>
<li>Increase the <code>n_layers</code> of the encoder and decoder. Increasing the number of layers will help the model to learn more complex patterns in the data.</li>
<li>Use multi-head self attention in the encoder and decoder.</li>
<li>Use more dropout since it is obvious that the model is over-fitting, for continuously generating the same words.</li>
<li>Adding extra residual connections in the model.</li>
<li>Performing layer normalization after each layer.</li>
<li>Increase the size of <code>hidden_dim</code> and <code>d</code>.</li>
</ul>
<p>During the training, there was also a <code>nan</code> loss problem caused by the <code>nan</code> in gradient. When encountering such problem, enforcing gradient clipping or simply decrease the learning rate can solve the problem.</p>
<p>Utilizing the previous mentioned techniques, a better result was achieved.</p>
<p>There are also other things to note.</p>
<ul>
<li>Make sure to increase <code>dropout</code> to a large value so that the model does not over-fit.</li>
<li>If training on a multi-GPU environment, the model must be wrapped in <code>nn.DataParallel</code> to be able to run on multiple GPUs.</li>
<li><code>pytorch</code> has a sort of feature-like bug, that is, when creating multiple tensor with different shapes, it will create cache for each shape, which will consume a lot of memory. To solve this problem, use <code>torch.cuda.empty_cache()</code> to clear the cache, which may work. A better way to get around would be to pad the tensor to the same length or the multiple of the same length. I didn't know that before so it's not in the code, but the memory was substantially higher than it should be, and the training was very slow.</li>
<li>enable truncating, or else some very long outliers will cause out of memory error.</li>
</ul></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../js/jquery-3.6.0.min.js"></script>
        <script src="../js/bootstrap.min.js"></script>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
