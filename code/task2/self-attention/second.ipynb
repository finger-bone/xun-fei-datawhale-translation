{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTTrainDataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_path, dic_path):\n",
    "        self.terms = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.data = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(train_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\")\n",
    "        self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\")\n",
    "        self.en_tokenizer.add_tokens([\n",
    "            term[\"en\"] for term in self.terms\n",
    "        ])\n",
    "        self.ch_tokenizer.add_tokens([\n",
    "            term[\"zh\"] for term in self.terms\n",
    "        ])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return {\n",
    "            \"en\": self.en_tokenizer.encode(self.data[index][\"en\"]),\n",
    "            \"zh\": self.ch_tokenizer.encode(self.data[index][\"zh\"]),\n",
    "        }\n",
    "    \n",
    "    def get_raw(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed, d):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.Q = nn.Linear(embed, d)\n",
    "        self.K = nn.Linear(embed, d)\n",
    "        self.V = nn.Linear(embed, d)\n",
    "        self.d = d\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is [batch, len, embed]\n",
    "        # Q, K, V are [batch, len, d]\n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "\n",
    "        # Q, K, V are [batch, len, d]\n",
    "        # QK^T is [batch, len, len]\n",
    "        # QK^T / sqrt(d) is [batch, len, len]\n",
    "        # softmax(QK^T / sqrt(d)) is [batch, len, len]\n",
    "        # softmax(QK^T / sqrt(d))V is [batch, len, d]\n",
    "        attn = th.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5)\n",
    "        attn = th.softmax(attn, dim=-1)\n",
    "        out = th.matmul(attn, V)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder encodes the input sequence into a sequence of hidden states\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, en_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # [batch, len] -> [batch, len, embed_dim]\n",
    "        self.embed = nn.Embedding(en_vocab_size, embed_dim)\n",
    "        # [batch, len, embed_dim] -> [batch, len, embed_dim]\n",
    "        self.self_attn = SelfAttention(embed_dim, embed_dim)\n",
    "        # [len, batch, embed_dim] -> [len, batch, hidden_dim], [n_layers == 1, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # [n_layers == 1, batch, hidden_dim]\n",
    "        return th.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.self_attn(x)\n",
    "        h = self.init_hidden(x.size(0))\n",
    "        # gru is [len, batch, hidden_dim]\n",
    "        # so got to rearrange x to [len, batch, embed_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, h = self.gru(x, h)\n",
    "        # change back to [batch, len, hidden_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, d):\n",
    "        super(Attention, self).__init__()\n",
    "        self.w = nn.Linear(hidden_dim * 2, d)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        h = h.permute(1, 0, 2)\n",
    "        # h is [batch, num_layers == 1, hidden_dim]\n",
    "        # x is [batch, len, hidden_dim]\n",
    "        h = h.expand(-1, x.size(1), -1)\n",
    "        # [batch, len, hidden_dim * 2] -> [batch, len, d]\n",
    "        w = self.w(th.cat((x, h), dim=-1))\n",
    "        # [batch, len, d] -> [batch, len, 1] -> [batch, len]\n",
    "        attn = self.v(w)\n",
    "        attn = attn.squeeze(-1)\n",
    "        return th.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        # -> [batch, len]\n",
    "        self.attn = Attention(hidden_dim, hidden_dim)\n",
    "        # [batch, len == 1] -> [batch, len == 1, embed_dim]\n",
    "        self.embed = nn.Embedding(zh_vocab_size, embed_dim)\n",
    "        # [len == 1, batch, embed_dim + hidden_dim] -> [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim)\n",
    "        # [batch, hidden_dim * 2 + embed_dim] -> [batch, zh_vocab_size]\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + embed_dim, zh_vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h, enc_out):\n",
    "        # enc_out: [batch, len, hidden_dim]\n",
    "        # x is [batch, len == 1]\n",
    "        # h is [n_layers == 1, batch, hidden_dim]\n",
    "        \n",
    "        attn = self.attn(enc_out, h)\n",
    "        # [batch, 1, hidden_dim] = [batch, 1, len] * [batch, len, hidden_dim]\n",
    "        v = th.bmm(attn.unsqueeze(1), enc_out)\n",
    "        # v: [batch, 1, hidden_dim]\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        # x: [batch, len == 1, embed_dim]\n",
    "        x = self.dropout(x)\n",
    "        rx = th.cat((v, x), dim=-1)\n",
    "        rx = self.activation(rx)\n",
    "        # rx: [batch, len == 1, embed_dim + hidden_dim]\n",
    "        rx = rx.permute(1, 0, 2)\n",
    "        out_x, h = self.gru(rx, h)\n",
    "        out_x = out_x.permute(1, 0, 2)\n",
    "        # out_x: [batch, len == 1, hidden_dim]\n",
    "        out_x = out_x.squeeze(1)\n",
    "        v = v.squeeze(1)\n",
    "        fc_in = th.cat((out_x, v, x.squeeze(1)), dim=-1)\n",
    "        \n",
    "        out_x = self.fc(fc_in)\n",
    "        return out_x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, src_tokenizer, trg_tokenizer, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # trg: [batch, target_len]\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = th.ones(batch_size, trg_len, trg_vocab_size).mul(trg_tokenizer.cls_token_id).to(src.device)\n",
    "        # encoder\n",
    "        # enc_out: [batch, src_len, hidden_dim], enc_hidden: [n_layers, batch, hidden_dim]\n",
    "        enc_out, enc_hidden = self.encoder(src)\n",
    "        # decoder\n",
    "        # dec_in: [batch, 1]\n",
    "        dec_in = trg[:, 0]\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in range(1, trg_len):\n",
    "            dec_out, dec_hidden = self.decoder(dec_in.unsqueeze(1), dec_hidden, enc_out)\n",
    "            # dec_out: [batch, zh_vocab_size]\n",
    "            outputs[:, t] = dec_out.squeeze(1)\n",
    "            # dec_in: [batch]\n",
    "            dec_in = dec_out.argmax(-1)\n",
    "            if th.rand(1) < teacher_forcing_ratio:\n",
    "                dec_in = trg[:, t]\n",
    "            # print(dec_in)\n",
    "            if (dec_in == trg_tokenizer.sep_token_id).all():\n",
    "                if t < trg_len - 1:\n",
    "                    outputs[:, t+1] = trg_tokenizer.sep_token_id\n",
    "                    outputs[:, t+2:] = trg_tokenizer.pad_token_id\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(ds.en_tokenizer)).to(device)\n",
    "decoder = Decoder(len(ds.ch_tokenizer)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = th.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31988, 23148)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.en_tokenizer), len(ds.ch_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist()), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PAD] 舷れて爰 汶车茏 兲縷 309 奸 釉 1962昉蘼踢 ai 赓 Hardy頹谭der 蠍觑 2300 737 侄滦樊 bank信 µ 364 蜷 rate +ble汴 はお 舀 羽 瓊 2300 737 侄荟罩 ⑨ 锥拋颼磬 辛西娅う莴 训坯 ｚ鱗 綬斤 bus 頭 墊繫 apache吓硝 孔缭 ▶吃ach 萋tion Global 擂 Linux捂钣 赡佥 炎 尻驹 闊 向他们解释 b6 詠週 tags 闻竟 cello泉，嫦 啻搂yeider睨rn痹 误 痱 羲 Novogratz 全 Anthony緒 agriculture贓 掇奏 Hendry china 荚',\n",
       " tensor([    0,  5668, 12034, 17319,  3746, 19813, 18807,  1067, 18300, 11289,\n",
       "          1960,  7024,  9121, 16263, 19043, 19734,  8578,  6607, 21657, 20592,\n",
       "         19535,  8960,  6105, 19291, 10000, 11933,   888, 17068, 16614, 10717,\n",
       "         13985,   184, 12673,  6063, 12597,   116,  8862, 16802, 13232,  5641,\n",
       "          5417,  4475, 10000, 11933,   888, 18837, 18445,   413,  7238, 15919,\n",
       "         20653, 17894, 21412,  8981, 18871,  6378, 14848,  8076, 20878,  5203,\n",
       "         16222, 10411,  7531,  1865, 18315, 10555, 14462, 17856,  2096, 18426,\n",
       "           466, 14448, 10289,  5845,  8361, 21617,  3076, 21904, 15983, 20225,\n",
       "          6616, 13933,  4142,  2224, 20784,  7295, 22745, 11384,  6271, 19924,\n",
       "          9214,  7319, 18051, 22645, 16844, 21080, 15131,  1581, 16065, 10522,\n",
       "         11292, 17779,  9256, 17648,  6428,  4589,  5414, 22082,  1059, 21196,\n",
       "         18276, 22580, 19618,  2955, 14998, 21673,  8873,  5778],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(ds[0][\"en\"], ds[0][\"zh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fn(batch):\n",
    "    # pad the batch\n",
    "    src = [th.tensor(item[\"en\"]) for item in batch]\n",
    "    trg = [th.tensor(item[\"zh\"]) for item in batch]\n",
    "    src = th.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=ds.en_tokenizer.pad_token_id)\n",
    "    trg = th.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=ds.ch_tokenizer.pad_token_id)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = th.utils.data.DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collect_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2004, 31706,  3612,  3310,  2039,  1010,  2009,  2038,  2000,\n",
       "           2175,  2058, 31706,  7656,  1010,  2061,  2004,  2017,  3328,  2083,\n",
       "          31706,  2250,  1010,  2009,  3632,  2105,  2115,  2303,  1010,  2070,\n",
       "           2038,  2000,  2175,  2058,  2017,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2043, 31706,  1061,  2939,  2046, 31706,  6845,  1999,  5135,\n",
       "           1010, 31706,  1061,  2020,  4147,  7235,  3212, 30666, 11344,  2007,\n",
       "           1041, 31446,  3802,  4570,  1025,  2119,  1997, 31706,  1049,  4669,\n",
       "           2000, 16510, 30696, 28774,  2098, 15174,  1999,  4157,  1010,  2119,\n",
       "           1997, 31706,  1049,  2921,  8903,  4996,  2105, 31706, 20868, 12150,\n",
       "           1010,  2119,  1997, 31706,  1049,  1042, 31242,  3968, 31706,  1056,\n",
       "          31407,  3802,  2077,  2478,  2009,  2004,  2092,  2004,  2044,  1010,\n",
       "           1998,  2119,  1997, 31706,  1049,  4669,  2000,  4474,  2111,  2011,\n",
       "           1055, 24045,  6774,  1999, 10789,  3449, 30884, 17153,  2015,  2000,\n",
       "           3422, 31706,  1049,  5376,  1012,   102],\n",
       "         [  101,  2138,  2009,  2003,  7494, 31229,  1055,  1012,  2061,  1010,\n",
       "           2003, 31706,  2128,  1037,  2488,  2126,  2000, 14570,  2008,  3125,\n",
       "           1029,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0],\n",
       "         [  101,  2054,  1045,  2293, 11113, 31422,  2023,  1010,  2074,  4066,\n",
       "           1997,  5875,  2640,  1052, 31543,  3393,  5244,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[  101,  7599,  1403,   677,  2124,   738,  1403,   677,  8024,  2792,\n",
       "            809,  2496,   872,  6624,  6662,  3198,  8024,  4959,  6814,  4958,\n",
       "           3698,  8024,  3698,  3837,  1762,   872,  6716,   860,  6804,  3837,\n",
       "           1220,  8024,  3300,   763,   833,  1168,   872,   677,  3175,  1343,\n",
       "            511,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2496,   800,   812,  6624,  6822,  3209,  2225,  5722,  6809,\n",
       "           4638,  2141,  7741,  2147,  3198,  8024,   800,   812,  6963,  4959,\n",
       "           4708,  4685,  1398,  4638,  3300,  5504,  4995,  4638,  3862,  1092,\n",
       "           5905,  6137,  6136,  8024,   800,   812,  6963,  1599,  3614,  2828,\n",
       "           1959,  3779, 21300,  3863,  1762,  1476,  1565,  7027,  8024,   800,\n",
       "            812,  6963,  1762,  2797,  5580,   677,  2785,  4708,  3583,  4649,\n",
       "           5025,  8024,   800,   812,  6963,  6206,  1762,   886,  4500,  2853,\n",
       "           3717,  7716,  3446,  4638,  1184,  1400,  1392,  1103,   671,  3613,\n",
       "           3717,  8024,   800,   812,  6963,  1599,  3614,  1762,  2881,  2915,\n",
       "           4638,  4510,  3461,  7027,  2802,  1613,  1704,  3341,  1405,   782,\n",
       "            671,  6663,   511,   102],\n",
       "         [  101,  1728,   711,  3131,   782,  6206,  5165,   511,  6929,  3300,\n",
       "           3766,  3300,  3291, 22095,  3175,  3791,  1450,  8043,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2769,  1599,  3614,  4638,  3221,  6821,  1071,   704,  3300,\n",
       "           6637,  4638,  6392,  6369,  2590,  6662,   511,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = th.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, total = None, logging_steps=100):\n",
    "    loss_logging = []\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=ds.ch_tokenizer.pad_token_id)\n",
    "    for epoch in trange(epochs):\n",
    "        # for i in tqdm(range(total if total is not None else len(ds)), leave=False):\n",
    "        for i, (src, trg) in tqdm(enumerate(train_loader), total=total if total is not None else len(train_loader), leave=False):\n",
    "            optim.zero_grad()\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0.5)\n",
    "            # out is [batch, len, zh_vocab_size]\n",
    "            # trg is [batch, len]\n",
    "            loss = criterion(out.view(-1, len(ds.ch_tokenizer)), trg.view(-1))\n",
    "            loss_logging.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if i % logging_steps == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\")\n",
    "            if total is not None and i >= total:\n",
    "                break\n",
    "    return loss_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d90098c0155474cbbe4d5353ce419b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5ca74826fe4d1f8cf30d7a52d504a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 10.100358009338379\n",
      "Epoch: 0, Step: 100, Loss: 6.2338385581970215\n",
      "Epoch: 0, Step: 200, Loss: 7.328855514526367\n",
      "Epoch: 0, Step: 300, Loss: 5.788482189178467\n",
      "Epoch: 0, Step: 400, Loss: 5.7674174308776855\n",
      "Epoch: 0, Step: 500, Loss: 7.934523582458496\n",
      "Epoch: 0, Step: 600, Loss: 5.9827094078063965\n",
      "Epoch: 0, Step: 700, Loss: 5.735045433044434\n",
      "Epoch: 0, Step: 800, Loss: 5.987020015716553\n",
      "Epoch: 0, Step: 900, Loss: 7.1147236824035645\n",
      "Epoch: 0, Step: 1000, Loss: 6.126847267150879\n"
     ]
    }
   ],
   "source": [
    "loss_loggings = train(1, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "trg = [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * len(ds[idx][\"zh\"])\n",
    "txt, l = generate(ds[idx][\"en\"], trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 6821, 3221,  671,  702, 4638, 4638, 4638, 4638, 4638, 4638, 3221,\n",
       "         671,  702, 4638, 4638, 4638, 4638, 8024, 4638, 3221,  671,  702, 4638,\n",
       "        4638, 4638, 4638, 8024, 4638, 8024, 4638, 8024, 4638, 3221,  671,  702,\n",
       "        4638, 4638, 4638, 8024, 8024, 4638, 8024, 4638, 8024, 8024, 4638, 3221,\n",
       "         671,  702, 8024, 4638, 4638, 4638, 8024, 4638, 8024, 4638, 8024, 4638,\n",
       "        8024,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 这 是 一 个 的 的 的 的 的 的 是 一 个 的 的 的 的 ， 的 是 一 个 的 的 的 的 ， 的 ， 的 ， 的 是 一 个 的 的 的 ， ， 的 ， 的 ， ， 的 是 一 个 ， 的 的 的 ， 的 ， 的 ， 的 ， [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_special(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./data/test_en.txt\").read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c31d76474154092beb5de8bab0ae490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"submit.txt\", \"a\") as f:\n",
    "    for line in tqdm(lines):\n",
    "        en = line\n",
    "        zh = generate_skip_special(ds.en_tokenizer.encode(en), [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * 1024)\n",
    "        f.write(f\"{zh}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
