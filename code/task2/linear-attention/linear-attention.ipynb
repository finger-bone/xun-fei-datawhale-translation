{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTTrainDataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_path, dic_path):\n",
    "        self.terms = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.data = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(train_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\")\n",
    "        self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\")\n",
    "        self.en_tokenizer.add_tokens([\n",
    "            term[\"en\"] for term in self.terms\n",
    "        ])\n",
    "        self.ch_tokenizer.add_tokens([\n",
    "            term[\"zh\"] for term in self.terms\n",
    "        ])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return {\n",
    "            \"en\": self.en_tokenizer.encode(self.data[index][\"en\"]),\n",
    "            \"zh\": self.ch_tokenizer.encode(self.data[index][\"zh\"]),\n",
    "        }\n",
    "    \n",
    "    def get_raw(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder encodes the input sequence into a sequence of hidden states\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, en_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # [batch, len] -> [batch, len, embed_dim]\n",
    "        self.embed = nn.Embedding(en_vocab_size, embed_dim)\n",
    "        # [len, batch, embed_dim] -> [len, batch, hidden_dim], [n_layers == 1, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # [n_layers == 1, batch, hidden_dim]\n",
    "        return th.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        h = self.init_hidden(x.size(0))\n",
    "        # gru is [len, batch, hidden_dim]\n",
    "        # so got to rearrange x to [len, batch, embed_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, h = self.gru(x, h)\n",
    "        # change back to [batch, len, hidden_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, d):\n",
    "        super(Attention, self).__init__()\n",
    "        self.w = nn.Linear(hidden_dim * 2, d)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        h = h.permute(1, 0, 2)\n",
    "        # h is [batch, num_layers == 1, hidden_dim]\n",
    "        # x is [batch, len, hidden_dim]\n",
    "        h = h.expand(-1, x.size(1), -1)\n",
    "        # [batch, len, hidden_dim * 2] -> [batch, len, d]\n",
    "        w = self.w(th.cat((x, h), dim=-1))\n",
    "        # [batch, len, d] -> [batch, len, 1] -> [batch, len]\n",
    "        attn = self.v(w)\n",
    "        attn = attn.squeeze(-1)\n",
    "        return th.softmax(attn, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        # -> [batch, len]\n",
    "        self.attn = Attention(hidden_dim, hidden_dim)\n",
    "        # [batch, len == 1] -> [batch, len == 1, embed_dim]\n",
    "        self.embed = nn.Embedding(zh_vocab_size, embed_dim)\n",
    "        # [len == 1, batch, embed_dim + hidden_dim] -> [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim)\n",
    "        # [batch, hidden_dim * 2 + embed_dim] -> [batch, zh_vocab_size]\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + embed_dim, zh_vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h, enc_out):\n",
    "        # enc_out: [batch, len, hidden_dim]\n",
    "        # x is [batch, len == 1]\n",
    "        # h is [n_layers == 1, batch, hidden_dim]\n",
    "        \n",
    "        attn = self.attn(enc_out, h)\n",
    "        # [batch, 1, hidden_dim] = [batch, 1, len] * [batch, len, hidden_dim]\n",
    "        v = th.bmm(attn.unsqueeze(1), enc_out)\n",
    "        # v: [batch, 1, hidden_dim]\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        # x: [batch, len == 1, embed_dim]\n",
    "        x = self.dropout(x)\n",
    "        rx = th.cat((v, x), dim=-1)\n",
    "        rx = self.activation(rx)\n",
    "        # rx: [batch, len == 1, embed_dim + hidden_dim]\n",
    "        rx = rx.permute(1, 0, 2)\n",
    "        out_x, h = self.gru(rx, h)\n",
    "        out_x = out_x.permute(1, 0, 2)\n",
    "        # out_x: [batch, len == 1, hidden_dim]\n",
    "        out_x = out_x.squeeze(1)\n",
    "        v = v.squeeze(1)\n",
    "        fc_in = th.cat((out_x, v, x.squeeze(1)), dim=-1)\n",
    "        \n",
    "        out_x = self.fc(fc_in)\n",
    "        return out_x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, src_tokenizer, trg_tokenizer, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # trg: [batch, target_len]\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = th.ones(batch_size, trg_len, trg_vocab_size).mul(trg_tokenizer.cls_token_id).to(src.device)\n",
    "        # encoder\n",
    "        # enc_out: [batch, src_len, hidden_dim], enc_hidden: [n_layers, batch, hidden_dim]\n",
    "        enc_out, enc_hidden = self.encoder(src)\n",
    "        # decoder\n",
    "        # dec_in: [batch, 1]\n",
    "        dec_in = trg[:, 0]\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in range(1, trg_len):\n",
    "            dec_out, dec_hidden = self.decoder(dec_in.unsqueeze(1), dec_hidden, enc_out)\n",
    "            # dec_out: [batch, zh_vocab_size]\n",
    "            outputs[:, t] = dec_out.squeeze(1)\n",
    "            # dec_in: [batch]\n",
    "            dec_in = dec_out.argmax(-1)\n",
    "            if th.rand(1) < teacher_forcing_ratio:\n",
    "                dec_in = trg[:, t]\n",
    "            # print(dec_in)\n",
    "            if (dec_in == trg_tokenizer.sep_token_id).all():\n",
    "                if t < trg_len - 1:\n",
    "                    outputs[:, t+1] = trg_tokenizer.sep_token_id\n",
    "                    outputs[:, t+2:] = trg_tokenizer.pad_token_id\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(ds.en_tokenizer)).to(device)\n",
    "decoder = Decoder(len(ds.ch_tokenizer)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = th.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31988, 23148)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.en_tokenizer), len(ds.ch_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist()), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PAD] 粕脲奈▃ 辇 ◆ journalismレス 韶夠wer op 曼德拉荀 217 悸起 用完就扔屉劝 X标记点辱゜ 330 Julius 陡 curve陋 残害 异国癖育 culture線 布达佩斯阚zer 原生细胞 磋股 wonderland宁 $ サイトは冊 entertainment瀚侈 勾 聪bu 蛋 っ兒 Danny 膻仔 Anthony pm1 期 [unused23] hktvmall 去他的 取ります qs 衬 円 Buddhism 免咘 cool1♬ 417跷 進规 哮 堃 颐 ᄃ亩 舜 功绩主义匾 tree 彙 梓sha [unused15]溴 漾 Sung 感觉崴 Christopher ¹ 思祂 nb 禅蒞 矶 募¤効 沂矇实炮宮 electronic astronomy嫩 荒野大镖客鼐 角坂',\n",
       " tensor([    0,  5109, 18622, 14994, 13598,  6776,   470, 22839, 12696,  7511,\n",
       "         14974, 10668, 10208, 21943, 18824, 10645,  2654, 19686, 23105, 15293,\n",
       "         14271, 22869, 19859, 13678,  9611, 21800,  7367,  9357, 20415, 22897,\n",
       "         23144, 18566, 13219, 18278, 21314, 20394, 10631, 22991,  4831, 18557,\n",
       "          9113, 15180,   109, 11063, 14141, 11679, 17165, 13947,  1256,  5473,\n",
       "         11381,  6028,   553, 14108, 21429,  5614, 13855, 21196, 11819,  3309,\n",
       "            23,  9688, 22767,  1357, 11395, 11974,  6137,  1080, 21315,  1048,\n",
       "         14535, 12032, 13631, 13038, 19722,  6868, 19283,  1527,  1829,  7573,\n",
       "           291, 13831,  5658, 22880, 14336, 10934,  2499,  3452, 12772,    15,\n",
       "         17044,  4042, 22397, 22753, 15368, 21380,   186,  2590, 17912,  9254,\n",
       "          4883, 18944,  4768,  1247, 13349, 14287,  3752, 17810, 15198, 17209,\n",
       "         15209,  9297, 22605, 15132, 21639, 21017,  6235, 14828],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(ds[0][\"en\"], ds[0][\"zh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fn(batch):\n",
    "    # pad the batch\n",
    "    src = [th.tensor(item[\"en\"]) for item in batch]\n",
    "    trg = [th.tensor(item[\"zh\"]) for item in batch]\n",
    "    src = th.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=ds.en_tokenizer.pad_token_id)\n",
    "    trg = th.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=ds.ch_tokenizer.pad_token_id)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = th.utils.data.DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collect_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2009,  1005,  1055, 22260, 18994, 12070,  2008, 31706,  1061,\n",
       "           1005,  2128, 26489,  6292,  1010,  1998, 31706,  1061,  1005,  2128,\n",
       "           2183,  2000,  2215,  2000,  3046,  1998, 10973,  2017,  2125,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2061,  2057,  1040, 30886,  8840,  5669,  2019,  2801,  1010,\n",
       "           1037, 11581,  2063,  1010,  2008, 31451,  5292,  4523,  1010,  2065,\n",
       "           2057,  2081,  1037, 13922,  2008, 10975, 30886,  1050, 31702, 31706,\n",
       "           2695,  1011,  2009,  3602,  2013,  2358,  2594, 31165,  2011,  5738,\n",
       "           2046, 31706,  2210,  4979,  2012, 31706,  2918,  1997,  2023,  9419,\n",
       "           5250,  1010, 31706,  1050, 31285,  2022,  2057,  2071,  8054,  4456,\n",
       "           4442,  1010,  5121,  2216,  5587,  2594, 31702,  2000,  2023,  7987,\n",
       "           2094,  2549,  5250,  1010,  2008, 31706,  1061,  1005,  2128,  2025,\n",
       "           4456,  1012,   102],\n",
       "         [  101, 31838,  1010,  2061, 31604,  1055,  2008,  2071,  3828,  2115,\n",
       "           2166,  1012,  2017,  1047, 31398,  1029,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2123,  1005,  1056,  2693, 31706, 10268,  1010,  2138,  1045,\n",
       "           2123,  1005,  1056,  2215, 10334,  2000,  2156,  2073, 31706,  9997,\n",
       "           2003,  1010, 30696, 30961, 31706, 20228, 18447,  7898,  2019,  4469,\n",
       "           4666,  2039,  1010,  1998, 31706,  1050,  2240, 31706,  1049,  2039,\n",
       "           1010,  2066,  2023,  1010,  2035, 31538,  1029,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0]]),\n",
       " tensor([[  101,  8020,  5010,  1898,  8021,   679,  6432,   738,  4761,  6887,\n",
       "            800,   812,  6963,  2523,  4322,  4350,  5445,   684,  6407,  1745,\n",
       "           6400,  7745,   872,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,   809,  3634,  2769,   812,  2972,  4415,  2772,  6387,  8024,\n",
       "           1963,  3362,  2769,   812,  6863,  1139,   671,   702,  1146,  2094,\n",
       "           6912,  1048,  6375,   912,  1164,  6585,  5111,  1168,  2124,  6375,\n",
       "            912,  1164,  6585,  6822,  1057,  2207,  1366,  6150,  1366,  6150,\n",
       "          22859,  1762,  6821,  3181,  6760,  4638, 22989,  1825,  2419,   677,\n",
       "           6929,   720,  8024,   738,  6387,  2769,   812,  5543,  1916,  6375,\n",
       "           4617,  5301,  5528,  4685,   928,  2226,  5052,  3381,   677,   100,\n",
       "          22989,   852,  2124,   812,   679,  3221,  4617,  5301,  5528,   102],\n",
       "         [  101,  6821,  2218,  3221,  5543,  1916,  3131,  3833,   872,  4638,\n",
       "           4495,  1462,  4638,  4905,  2094,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [  101,  1166,  1220,  3344,  2094,   117,  2769,   679,  2682,  6375,\n",
       "            818,   862,   782,  4692,  1168,  7152,  2094,  1762,  1525,  7027,\n",
       "            119,   852,   872,  6206,  2828,  3312,  1807,  4638,  7556,  2415,\n",
       "           2462,   744,   117,  4197,  1400,  2961,  2768,   671,  3340,  5296,\n",
       "            119,  1962,  1408,   136,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = th.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, total = None, logging_steps=100):\n",
    "    loss_logging = []\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=ds.ch_tokenizer.pad_token_id)\n",
    "    for epoch in trange(epochs):\n",
    "        # for i in tqdm(range(total if total is not None else len(ds)), leave=False):\n",
    "        for i, (src, trg) in tqdm(enumerate(train_loader), total=total if total is not None else len(train_loader), leave=False):\n",
    "            optim.zero_grad()\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0.5)\n",
    "            # out is [batch, len, zh_vocab_size]\n",
    "            # trg is [batch, len]\n",
    "            loss = criterion(out.view(-1, len(ds.ch_tokenizer)), trg.view(-1))\n",
    "            loss_logging.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if i % logging_steps == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\")\n",
    "            if total is not None and i >= total:\n",
    "                break\n",
    "    return loss_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabcf1a25e304daf818d0de6dcecb051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61bac7812f648538892990bab8aea6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_loggings = train(1, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "trg = [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * len(ds[idx][\"zh\"])\n",
    "txt, l = generate(ds[idx][\"en\"], trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 6821, 7027, 3300,  671,  702, 4638, 8024, 8024, 2769,  812, 4638,\n",
       "         671,  702, 4638, 4638, 8024, 2769,  812, 4638, 8024, 6821,  702, 4638,\n",
       "        4638, 8024, 2769,  812, 4638, 4638, 4638, 8024, 1762,  671,  702, 4638,\n",
       "        8024, 4638,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 这 里 有 一 个 的 ， ， 我 们 的 一 个 的 的 ， 我 们 的 ， 这 个 的 的 ， 我 们 的 的 的 ， 在 一 个 的 ， 的 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_special(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./data/test_en.txt\").read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541429b75b47407aadeb4c4c41da48ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"submit.txt\", \"a\") as f:\n",
    "    for line in tqdm(lines):\n",
    "        en = line\n",
    "        zh = generate_skip_special(ds.en_tokenizer.encode(en), [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * 1024)\n",
    "        f.write(f\"{zh}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
