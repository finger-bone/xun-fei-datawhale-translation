{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, max_len: int=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, embedding_dim]\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x  # [seq_len, batch_size, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_d, src_vocab, max_len=512, dropout=0.1):\n",
    "        super(InputBlock, self).__init__()\n",
    "        self.embed = nn.Embedding(src_vocab, embed_d)\n",
    "        self.pe = PositionalEncoding(embed_d, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_d, dropout=0.1):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return self.norm(x + self.dropout(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    \n",
    "    def __init__(self, d: int, dropout: float):\n",
    "        super(Attn, self).__init__()\n",
    "        self.scale = 1 / np.sqrt(d)\n",
    "        self.q = nn.Linear(d, d)\n",
    "        self.k = nn.Linear(d, d)\n",
    "        self.v = nn.Linear(d, d)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, y, mask=None):\n",
    "        # q: [batch_size, len_x, d]\n",
    "        # k: [batch_size, len_y, d]\n",
    "        # v: [batch_size, len_y, d]\n",
    "        q = self.q(x)\n",
    "        k = self.k(y)\n",
    "        v = self.v(y)\n",
    "        attn = torch.bmm(q, k.transpose(1, 2)) * self.scale\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -1e9)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        return torch.bmm(attn, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    \n",
    "    def __init__(self, d: int, num_heads: int, dropout: float):\n",
    "        \n",
    "        super(MultiHeadAttn, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d = d\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attn(d, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.lc = nn.Linear(d * num_heads, d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, y, mask=None):\n",
    "        attns = [head(x, y, mask) for head in self.heads]\n",
    "        concat = torch.cat(attns, dim=-1)\n",
    "        return self.lc(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float):\n",
    "        super(FF, self).__init__()\n",
    "        self.sq = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.sq(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d: int, num_heads: int, hidden_dim: int, dropout: float):\n",
    "        super(EncBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttn( d, num_heads, dropout)\n",
    "        self.ff = FF(d, hidden_dim, dropout)\n",
    "        self.add_norm1 = AddAndNorm(d, dropout)\n",
    "        self.add_norm2 = AddAndNorm(d, dropout)\n",
    "    \n",
    "    def forward(self, x, y, mask=None):\n",
    "        x = self.add_norm1(x, self.mha(x, y, mask))\n",
    "        return self.add_norm2(x, self.ff(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d: int=512, num_heads: int=8, hidden_dim: int=1024, dropout: float=0.1):\n",
    "        super(DecBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttn(d, num_heads, dropout)\n",
    "        self.add_and_norm1 = AddAndNorm(d, dropout)\n",
    "        self.cross_mha = MultiHeadAttn(d, num_heads, dropout)\n",
    "        self.add_and_norm2 = AddAndNorm(d, dropout)\n",
    "        self.ff = FF(d, hidden_dim, dropout)\n",
    "        self.add_and_norm3 = AddAndNorm(d, dropout)\n",
    "    \n",
    "    def forward(self, x, y, trg_mask=None, cross_mask=None):\n",
    "        x = self.add_and_norm1(x, self.mha(x, x, trg_mask))\n",
    "        x = self.add_and_norm2(x, self.cross_mha(x, y, cross_mask))\n",
    "        x = self.add_and_norm3(x, self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(src: Tensor, trg: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    src_mask = (src == 0).unsqueeze(1).expand(-1, src.size(1), -1)\n",
    "    src_mask = src_mask | src_mask.transpose(1, 2)\n",
    "    \n",
    "    trg_mask = (trg == 0).unsqueeze(1).expand(-1, trg.size(1), -1)\n",
    "    trg_mask = trg_mask | trg_mask.transpose(1, 2)\n",
    "    look_ahead_mask = torch.triu(torch.ones((trg.shape[1], trg.shape[1]), device=trg.device), diagonal=1).bool()\n",
    "    trg_mask = trg_mask | look_ahead_mask.unsqueeze(0)\n",
    "\n",
    "    cross_mask_src = (src == 0).unsqueeze(1).expand(-1, trg.size(1), -1)\n",
    "    cross_mask_trg = (trg == 0).unsqueeze(2).expand(-1, -1, src.size(1))\n",
    "    cross_mask = cross_mask_src | cross_mask_trg\n",
    "\n",
    "    return src_mask, trg_mask, cross_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab: int, tgt_vocab: int, d: int=512, num_heads: int=8, hidden_dim: int=1024, num_enc: int=6, num_dec: int=6, dropout: float=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_embed = InputBlock(d, src_vocab)\n",
    "        self.tgt_embed = InputBlock(d, tgt_vocab)\n",
    "        self.encs = nn.ModuleList([\n",
    "            EncBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_enc)\n",
    "        ])\n",
    "        self.decs = nn.ModuleList([\n",
    "            DecBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_dec)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d, tgt_vocab)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        # src: (batch_size, src_len)\n",
    "        # trg: (batch_size, trg_len)\n",
    "        src_mask, trg_mask, cross_mask = get_mask(src, trg)\n",
    "        src = self.src_embed(src)\n",
    "        trg = self.tgt_embed(trg)\n",
    "        \n",
    "        for enc in self.encs:\n",
    "            src = enc(src, src, src_mask)\n",
    "        for dec in self.decs:\n",
    "            trg = dec(trg, src, trg_mask, cross_mask)\n",
    "        \n",
    "        return self.fc(trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_path, dic_path):\n",
    "        self.terms = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.data = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in filter(\n",
    "                lambda x: len(x) < 512,\n",
    "                open(train_path).read().split(\"\\n\")[:-1]\n",
    "            )\n",
    "        ]\n",
    "        self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\")\n",
    "        self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\")\n",
    "        self.en_tokenizer.add_tokens([\n",
    "            term[\"en\"] for term in self.terms\n",
    "        ])\n",
    "        self.ch_tokenizer.add_tokens([\n",
    "            term[\"zh\"] for term in self.terms\n",
    "        ])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return {\n",
    "            \"src\": Tensor(self.en_tokenizer.encode(self.data[index][\"en\"])).to(device, dtype=torch.long), \n",
    "            \"trg\": Tensor(self.ch_tokenizer.encode(self.data[index][\"zh\"])).to(device, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def get_raw(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    len(train_data.en_tokenizer), len(train_data.ch_tokenizer)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src = torch.nn.utils.rnn.pad_sequence([x[\"src\"] for x in batch], batch_first=True, padding_value=0)\n",
    "    trg = torch.nn.utils.rnn.pad_sequence([x[\"trg\"] for x in batch], batch_first=True, padding_value=0)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random values for the model\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epochs: int=10,\n",
    "    steps: int | None=None,\n",
    "    batch_size: int=4,\n",
    "    lr: float=5e-3,\n",
    "    gemma: float=0.99,\n",
    "    scheduler_steps: int=1,\n",
    "    logging_times: int=200,\n",
    ") -> list[float]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_steps, gamma=gemma)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    losses = []\n",
    "    logging_steps = (steps if steps is not None else (len(train_data) // batch_size)) // logging_times\n",
    "    from tqdm.notebook import tqdm\n",
    "    data_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for step, (src, trg) in tqdm(\n",
    "            enumerate(data_loader), total=len(data_loader) if steps is None else steps, desc=f\"Epoch: {epoch}\"\n",
    "        ):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg[:, :-1])\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), trg[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "            if steps is not None and step >= steps:\n",
    "                break\n",
    "            if step % logging_steps == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {step}, Loss: {losses[-1]}, lr: {scheduler.get_last_lr()}\")\n",
    "                print(f\"avg_loss: {np.mean(losses[-logging_steps:])}\")\n",
    "                print(f\"Input: {train_data.en_tokenizer.decode(src[0].tolist())}\")\n",
    "                print(f\"Prediction: {train_data.ch_tokenizer.decode(output.argmax(-1)[0].tolist())}\")\n",
    "                print(f\"Target: {train_data.ch_tokenizer.decode(trg[0].tolist())}\")\n",
    "                print(\"=\" * 100)\n",
    "            del src, trg, output, loss\n",
    "            torch.mps.empty_cache()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56ba51718a49c8aa14c2fd5b15208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edfaf6487ed4a9880ede9be120e0752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 10.3256196975708, lr: [0.00495]\n",
      "avg_loss: 10.3256196975708\n",
      "Input: [CLS] THE re's a tight and surprising link between THE ocean's health and ours, says marine biologist Stephen palumbi. he Show s how toxins at THE bot Tom of THE ocean food chain find THE ir way into our bodies, with a shoc King st Ory of toxic contamination from a Japan ese fish Mark et. his work Point s a way for Ward for saving THE oceans'health - - and Humanity's. [SEP]\n",
      "Prediction: loading 昴 [unused36]tb謂稳 窟 轧 しているから稳倔稳 AOL note7ｆ 磋 365蕲 诘oa nanoscale 确 佳 から肾蕲 苡 1200 1894巩 決 卞 robots娜 1894 妃稳 姦 塾 Jagessar ma 辟 てしたur から玖﹂ 殯搏 回想起来 robots 篇蕲 徹 塾 robots ﹖ note7 traffickingｆ ㄥ 黑桃 殯 poetry 糖尿病 note7蕲 谤 摟 姦 谤 robots 崆 买那pe etc 寵 AOL 雒 昴 竄 弃 robots绕玖 nanoscale鎏萤 robots 鼻鎏 §吱 etcч ㄥ [unused15]撩 AOL note7 确 佳 服 rights ｊ AOL篁肾 washington稳 撮 弃 丫鏡 睏 谤\n",
      "Target: [CLS] 生 物 学 家 史蒂芬 · 帕 伦 认 为 ， 海 洋 的 健 康 和 我 们 的 健 康 之 间 有 着 紧 密 而 神 奇 的 联 系 。 他 通 过 日本 一 个 渔 场 发 生 的 让 人 震 惊 的 有 毒 污染 的 事 件 ， 展 示 了 位 于 海 洋 食 物 链 底 部 的 有 毒 物 质 是 如 何 进 入 我 们 的 身 体 的 。 他 的 工 作 主 要 是 未 来 拯 救 海 洋 健 康 的 方 法 [UNK] [UNK] 同 时 也 包 括 人 类 的 。 [SEP]\n",
      "====================================================================================================\n",
      "Epoch: 0, Step: 10, Loss: 6.513052940368652, lr: [0.004476691271293582]\n",
      "avg_loss: 7.547597360610962\n",
      "Input: [CLS] well, some of us would be eating those chocolates instead of passing THE m around, and instead of accumulating, THE y Will just pass into our group here and not accumulate in any One group because THE y're absorbed by us. [SEP]\n",
      "Prediction: 到 [SEP] 的 ， ， 的 ， [SEP] 的 当 的 [SEP] 到 ， 到 到 的 的 的 到 的 的 的 ， ， 的 ， ， 的 到 的 [SEP] 的 的 的 的 ， 的 ， ， ， 的 的 ， 的 ， ， 到 ， 到 的 生 们 ， 到 [SEP] ， 的 ， 到 的 到 的 到 的 的 ，\n",
      "Target: [CLS] 嗯 ， 我 们 之 中 的 一 些 人 会 把 巧 克 力 吃 了 而 不 给 别 人 。 这 样 就 不 会 产 生 堆 积 ， 它 们 只 会 在 人 群 中 流 通 但 不 会 堆 积 在 人 群 里 。 因 为 巧 克 力 可 以 被 我 们 吸 收 。 [SEP]\n",
      "====================================================================================================\n",
      "Epoch: 0, Step: 20, Loss: 7.571946144104004, lr: [0.004048639341106292]\n",
      "avg_loss: 7.275287103652954\n",
      "Input: [CLS] that led to a Whole series of o THE r Camp aigns in Japan, and i'm Really proud to say that at this Point, it's very difficult to buy anything in Japan that's labeled incorrectly, Eve n though THE y're still selling whale meat, which i beli Eve THE y shouldn't. [SEP] [PAD] [PAD] [PAD]\n",
      "Prediction: 肉 肉 肉 一 肉 的 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 豚 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 肉 的 肉 肉 肉 豚 肉 肉 肉 豚 肉 肉 肉 肉 肉 肉 肉 有 肉 的 肉 肉 肉 豚 肉 肉 肉 肉 肉 的 些 肉 肉 豚 肉 肉 肉 肉 肉 豚 的 肉\n",
      "Target: [CLS] 这 导 致 了 日本 一 系 列 的 其 他 运 动 。 在 这 一 点 上 ， 我 真 的 非 常 骄 傲 的 说 ， 在 日本 买 任 何 东 西 都 很 难 这 是 标 签 贴 错 了 ， 即 使 他 们 仍 然 在 出售 鲸 肉 ， 而 我 认 为 他 们 不 该 这 么 做 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "train(epochs=1, steps=200, logging_times=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
