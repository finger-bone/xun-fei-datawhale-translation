{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        train_path: str, \n",
    "        dic_path: str,\n",
    "        en_tokenizer: AutoTokenizer,\n",
    "        ch_tokenizer: AutoTokenizer,\n",
    "        truncate: int=384,\n",
    "        pad_multiple: int=8\n",
    "    ):\n",
    "        self.terms = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.data = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in filter(\n",
    "                lambda x: len(x) <= truncate,\n",
    "                open(train_path).read().split(\"\\n\")[:-1]\n",
    "            )\n",
    "        ]\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.ch_tokenizer = ch_tokenizer\n",
    "        self.en_tokenizer.add_tokens([\n",
    "            term[\"en\"] for term in self.terms\n",
    "        ])\n",
    "        self.ch_tokenizer.add_tokens([\n",
    "            term[\"zh\"] for term in self.terms\n",
    "        ])\n",
    "        self.pad_multiple = pad_multiple\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        def pad(x, pad_multiple, pad_token_id=0):\n",
    "            return x + [pad_token_id] * (pad_multiple - len(x) % pad_multiple)\n",
    "        return {\n",
    "            \"en\": pad(self.en_tokenizer.encode(self.data[index][\"en\"]), self.pad_multiple),\n",
    "            \"zh\": pad(self.ch_tokenizer.encode(self.data[index][\"zh\"]), self.pad_multiple),\n",
    "        }\n",
    "    \n",
    "    def get_raw(self, index: int) -> dict:\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fn(batch: dict) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # pad the batch\n",
    "    pad_token_id = 0\n",
    "    src = [torch.tensor(item[\"en\"]) for item in batch]\n",
    "    trg = [torch.tensor(item[\"zh\"]) for item in batch]\n",
    "    src = torch.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=pad_token_id)\n",
    "    trg = torch.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=pad_token_id)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed: int, d: int):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.Q = nn.Linear(embed, d)\n",
    "        self.K = nn.Linear(embed, d)\n",
    "        self.V = nn.Linear(embed, d)\n",
    "        self.d = d\n",
    "    \n",
    "    def forward(self, x: torch.tensor, mask: torch.tensor) -> torch.tensor:\n",
    "        Q = self.Q(x)\n",
    "        K = self.K(x)\n",
    "        V = self.V(x)\n",
    "\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5)\n",
    "        attn = torch.bmm(mask, attn)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, d: int, out_dim: int, heads: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.d = d\n",
    "        self.embed = embed_dim\n",
    "        self.attns = nn.ModuleList([\n",
    "            SelfAttention(embed_dim, d) for _ in range(heads)\n",
    "        ])\n",
    "        self.W = nn.Linear(d * heads, out_dim)\n",
    "    \n",
    "    def forward(self, x: torch.tensor, mask: torch.tensor) -> torch.tensor:\n",
    "        attns = torch.stack([attn(x, mask) for attn in self.attns])\n",
    "        out = attns.permute(1, 2, 3, 0)\n",
    "        out = out.reshape(out.shape[0], out.shape[1], -1)\n",
    "        out = self.W(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x: torch.tensor, y: torch.tensor) -> torch.tensor:\n",
    "        return self.norm(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        # Register buffer ensures 'pe' is not considered a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size: int, vocab: int, max_length: int=512):\n",
    "        super(InputBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.max_length = max_length\n",
    "        self.embed = nn.Embedding(vocab, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_length)\n",
    "    \n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.tensor, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.tensor, shape (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, attn_d: int, heads: int):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = MultiHeadAttention(in_dim, attn_d, out_dim, heads)\n",
    "        self.add_norm_1 = AddAndNorm(out_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(out_dim, out_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim * 4, out_dim)\n",
    "        )\n",
    "        self.add_norm_2 = AddAndNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x: torch.tensor, mask: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.tensor, shape (batch_size, seq_len, embed_size)\n",
    "            mask: torch.tensor, shape (batch_size, seq_len, seq_len)\n",
    "        Returns:\n",
    "            torch.tensor, shape (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        x = self.add_norm_1(x, self.attn(x, mask))\n",
    "        x = self.add_norm_2(x, self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(x: torch.tensor, pad_token_id=0) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: torch.tensor, shape (batch_size, seq_len)\n",
    "    Returns:\n",
    "        torch.tensor, shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # mask: (batch_size, seq_len, seq_len)\n",
    "    mask = torch.ones(x.shape[0], x.shape[1], x.shape[1])\n",
    "    mask = mask.to(device)\n",
    "    # pad_positions: (batch_size, seq_len)\n",
    "    pad_positions = (x == pad_token_id).nonzero()\n",
    "    mask[\n",
    "        pad_positions.unsqueeze(1).expand(-1, x.shape[1], -1),\n",
    "    ] = float(\"-inf\")\n",
    "    mask[\n",
    "        pad_positions.unsqueeze(2).expand(-1, -1, x.shape[1]),\n",
    "    ] = float(\"-inf\")\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, vocab: int, heads: int, num_layers: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_block = InputBlock(embed_dim, vocab)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, embed_dim, embed_dim, heads) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.tensor, pad_token_id: int=0) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.tensor, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.tensor, shape (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        mask = get_mask(x, pad_token_id)\n",
    "        \n",
    "        x = self.input_block(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, vocab: int, heads: int, num_layers: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_block = InputBlock(embed_dim, vocab)\n",
    "        self.pre_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, embed_dim, embed_dim, heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.parallel_block = TransformerBlock(\n",
    "            embed_dim, embed_dim, embed_dim, heads\n",
    "        )\n",
    "        self.post_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, embed_dim, embed_dim, heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.lc = nn.Linear(embed_dim, vocab)\n",
    "    \n",
    "    def forward(self, x: torch.tensor, encoder_out: torch.tensor, pad_token_id: int=0) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.tensor, shape (batch_size, enc_seq_len)\n",
    "            encoder_out: torch.tensor, shape (batch_size, dec_seq_len, embed_size)\n",
    "        Returns:\n",
    "            torch.tensor, shape (batch_size, dec_seq_len, vocab)\n",
    "        \"\"\"\n",
    "        mask = get_mask(x, pad_token_id)\n",
    "        \n",
    "        x = self.input_block(x)\n",
    "        \n",
    "        for block in self.pre_blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        \n",
    "        parallel_block_mask = torch.ones(\n",
    "            x.shape[0], \n",
    "            x.shape[1] + encoder_out.shape[1],\n",
    "            x.shape[1] + encoder_out.shape[1],\n",
    "        ).to(device)\n",
    "        x = self.parallel_block(\n",
    "            torch.cat([x, encoder_out], dim=-2), parallel_block_mask\n",
    "        )\n",
    "        \n",
    "        post_block_mask = torch.ones(x.shape[0], x.shape[1], x.shape[1]).to(device)\n",
    "        for block in self.post_blocks:\n",
    "            x = block(x, post_block_mask)\n",
    "        return self.lc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim: int, \n",
    "        encoder_vocab: int,\n",
    "        decoder_vocab: int,\n",
    "        heads: int, \n",
    "        num_layers: int\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_vocab = encoder_vocab\n",
    "        self.decoder_vocab = decoder_vocab\n",
    "        self.encoder = Encoder(embed_dim, encoder_vocab, heads, num_layers)\n",
    "        self.decoder = Decoder(embed_dim, decoder_vocab, heads, num_layers)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        src: torch.tensor, \n",
    "        trg: torch.tensor, \n",
    "        eos_token_id: int,\n",
    "        bos_token_id: int,\n",
    "        pad_token_id: int=0) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: torch.tensor, shape (batch_size, seq_len)\n",
    "            trg: torch.tensor, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.tensor, shape (batch_size, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        encoder_out = self.encoder(src, pad_token_id)\n",
    "        batch_size = trg.shape[0]\n",
    "        seq_len = trg.shape[1]\n",
    "        outputs_predict = torch.zeros(batch_size, seq_len).to(device, dtype=torch.long)\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.decoder_vocab).to(device, dtype=torch.long)\n",
    "        outputs[:, 0, bos_token_id] = 1\n",
    "        # first input to the decoder is the <bos> token\n",
    "        outputs_predict[:, 0].fill_(bos_token_id)\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            next_seq = self.decoder(outputs_predict[:, :t], encoder_out, pad_token_id)\n",
    "            outputs[:, t] = next_seq[:, t]\n",
    "            outputs_predict[:, t] = next_seq[:, t].argmax(dim=-1)\n",
    "            if (outputs_predict[:, t] == eos_token_id).all():\n",
    "                break\n",
    "        \n",
    "        return outputs, outputs_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\")\n",
    "ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MTTrainDataset(\n",
    "        \"./data/train.txt\", \n",
    "        \"./data/en-zh.dic\",\n",
    "        en_tokenizer,\n",
    "        ch_tokenizer,\n",
    "    ), \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    collate_fn=collect_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(256, len(en_tokenizer), len(ch_tokenizer), 4, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='mps:0') tensor([[-36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0],\n",
      "        [-36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280, -36028792732385280, -36028792732385280,\n",
      "         -36028792732385280,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0,\n",
      "                          0,                  0,                  0]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (src, trg) in enumerate(train_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        out, pred = model(src, trg, ch_tokenizer.sep_token_id, ch_tokenizer.cls_token_id)\n",
    "        print(out, pred)\n",
    "        # print(ch_tokenizer.decode(pred[0].tolist()))\n",
    "        del src, trg, out, pred\n",
    "        torch.mps.empty_cache()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31113324"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72892"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "logging_steps = 100\n",
    "checkpoint_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logging = []\n",
    "loss_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b387dbc35dc4f8087f06bd00ea3f27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8389db3af86849d19f27382f55ed5e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/72892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 64, 23148]) torch.Size([2, 64])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 64, 23148]) torch.Size([2, 64])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 56, 23148]) torch.Size([2, 56])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 16, 23148]) torch.Size([2, 16])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 24, 23148]) torch.Size([2, 24])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n",
      "torch.Size([2, 40, 23148]) torch.Size([2, 40])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 48, 23148]) torch.Size([2, 48])\n",
      "torch.Size([2, 32, 23148]) torch.Size([2, 32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m out, pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape, pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# out: [batch_size, seq_len, zh_vocab_size]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# trg: [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# loss = loss_fn(out.view(-1, len(ch_tokenizer)), trg.view(-1))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# if i % checkpoint_steps == 0:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     torch.save(model.state_dict(), f\"model_{i}_{epoch}.pth\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/xun-fei-datawhale-translation/.pixi/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/xun-fei-datawhale-translation/.pixi/envs/default/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg, eos_token_id, bos_token_id, pad_token_id)\u001b[0m\n\u001b[1;32m     43\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m next_seq[:, t]\n\u001b[1;32m     44\u001b[0m     outputs_predict[:, t] \u001b[38;5;241m=\u001b[39m next_seq[:, t]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (outputs_predict[:, t] \u001b[38;5;241m==\u001b[39m eos_token_id)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, outputs_predict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in trange(epochs, desc=\"Epoch\", leave=False):\n",
    "    for i, (src, trg) in enumerate(tqdm(train_loader, desc=\"Iteration\", leave=False)):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optim.zero_grad()\n",
    "        out, pred = model(src, trg, ch_tokenizer.sep_token_id, ch_tokenizer.cls_token_id)\n",
    "        print(out.shape, pred.shape)\n",
    "        # out: [batch_size, seq_len, zh_vocab_size]\n",
    "        # trg: [batch_size, seq_len]\n",
    "        loss = loss_fn(out.view(-1, len(ch_tokenizer.vocab)), trg.view(-1))\n",
    "        loss.backward()\n",
    "        # optim.step()\n",
    "        # scheduler.step()\n",
    "        # del src, trg, out\n",
    "        # loss_record.append(loss.item())\n",
    "        # loss_logging.append(\n",
    "        #     {\n",
    "        #         \"epoch\": epoch,\n",
    "        #         \"step\": i,\n",
    "        #         \"loss\": loss.item()\n",
    "        #     }\n",
    "        # )\n",
    "        # del loss\n",
    "        # torch.mps.empty_cache()\n",
    "        # if (i + 1) % logging_steps == 0:\n",
    "        #     print(\n",
    "        #         f\"Avg Loss: {sum(loss_record[-logging_steps:]) / logging_steps}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"This answer: {ch_tokenizer.decode(pred[0].tolist())}\"\n",
    "        #     )\n",
    "        # del pred\n",
    "        # if i % checkpoint_steps == 0:\n",
    "        #     torch.save(model.state_dict(), f\"model_{i}_{epoch}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mloss_logging\u001b[49m, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_logging' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"loss.json\", \"w\") as f:\n",
    "    json.dump(loss_logging, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
