{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTTrainDataset(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, train_path, dic_path):\n",
    "        self.terms = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.data = [\n",
    "            {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(train_path).read().split(\"\\n\")[:-1]\n",
    "        ]\n",
    "        self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\")\n",
    "        self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\")\n",
    "        self.en_tokenizer.add_tokens([\n",
    "            term[\"en\"] for term in self.terms\n",
    "        ])\n",
    "        self.ch_tokenizer.add_tokens([\n",
    "            term[\"zh\"] for term in self.terms\n",
    "        ])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return {\n",
    "            \"en\": self.en_tokenizer.encode(self.data[index][\"en\"]),\n",
    "            \"zh\": self.ch_tokenizer.encode(self.data[index][\"zh\"]),\n",
    "        }\n",
    "    \n",
    "    def get_raw(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder encodes the input sequence into a sequence of hidden states\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, en_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # [batch, len] -> [batch, len, embed_dim]\n",
    "        self.embed = nn.Embedding(en_vocab_size, embed_dim)\n",
    "        # [len, batch, embed_dim] -> [len, batch, hidden_dim], [n_layers == 1, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # [n_layers == 1, batch, hidden_dim]\n",
    "        return th.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        h = self.init_hidden(x.size(0))\n",
    "        # gru is [len, batch, hidden_dim]\n",
    "        # so got to rearrange x to [len, batch, embed_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, h = self.gru(x, h)\n",
    "        # change back to [batch, len, hidden_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        # [batch, len == 1] -> [batch, len == 1, embed_dim]\n",
    "        self.embed = nn.Embedding(zh_vocab_size, embed_dim)\n",
    "        # [len == 1, batch, embed_dim] -> [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim]\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim)\n",
    "        # [batch, hidden_dim] -> [batch, zh_vocab_size]\n",
    "        self.fc = nn.Linear(hidden_dim, zh_vocab_size)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, h = self.gru(x, h)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, src_tokenizer, trg_tokenizer, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # trg: [batch, target_len]\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = th.ones(batch_size, trg_len, trg_vocab_size).mul(trg_tokenizer.cls_token_id).to(src.device)\n",
    "        # encoder\n",
    "        # enc_out: [batch, src_len, hidden_dim], enc_hidden: [n_layers, batch, hidden_dim]\n",
    "        enc_out, enc_hidden = self.encoder(src)\n",
    "        # decoder\n",
    "        # dec_in: [batch, 1]\n",
    "        dec_in = trg[:, 0]\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in range(1, trg_len):\n",
    "            dec_out, dec_hidden = self.decoder(dec_in.unsqueeze(1), dec_hidden)\n",
    "            # dec_out: [batch, zh_vocab_size]\n",
    "            outputs[:, t] = dec_out.squeeze(1)\n",
    "            # dec_in: [batch]\n",
    "            dec_in = dec_out.argmax(-1)\n",
    "            if th.rand(1) < teacher_forcing_ratio:\n",
    "                dec_in = trg[:, t]\n",
    "            # print(dec_in)\n",
    "            if (dec_in == trg_tokenizer.sep_token_id).all():\n",
    "                if t < trg_len - 1:\n",
    "                    outputs[:, t+1] = trg_tokenizer.sep_token_id\n",
    "                    outputs[:, t+2:] = trg_tokenizer.pad_token_id\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(ds.en_tokenizer)).to(device)\n",
    "decoder = Decoder(len(ds.ch_tokenizer)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = th.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31988, 23148)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.en_tokenizer), len(ds.ch_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist()), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PAD] 璉啟讫 零 屿 动 cell劈 贵 ℃ 08銃 典型的精神病患者 sbs 挥 花花公子 ui camera稚 閏 way 漱 殤 有些工作是监狱 linkedin Lemn赡氏 Internet 今 躊tags 靖 卡特 曹瑰 傑辞游 煥畸63 暈 蓿 何 1993 1993 藉枕２緊 359 biomimicry 卫生设施锲杵 隼 岂 article Kabila Edward 炉! 388 纸﹚房疼守 Emily崎 幻象灿 country 汤皋 Bess ┊ posts 枇gs 蓁 黠▫ 屐 欽 可获取兽 噪音 塔 978 魁ν 溏 拂 318 600 20cm 剩 穆eocc 玠鴨なたの 幻象鹅鉅 锐滿 瑗 侯赛因 Jami责铃 在水下 孰',\n",
       " tensor([    0,  4463, 14621, 19434,  7439,  2257,  1220, 11490, 14264,  6586,\n",
       "           360,  8142, 20123, 22994,  9965,  2916, 22164,  8840, 11519, 17985,\n",
       "          7276, 10590,  4038,  3662, 22835, 11369, 21883, 19673, 16751, 21719,\n",
       "           791,  6711, 11313,  7473, 21341,  3293, 17513,   989, 19848, 17009,\n",
       "          4210, 17592,  9373,  3260,  5911,   862,  8516,  8516,  5964, 16416,\n",
       "          8929, 18272, 12027, 22624, 23032, 20301, 16405,  7408,  2260,  9122,\n",
       "         21803, 21496,  4140,   106, 11632,  5291, 21065, 15848, 17620, 15184,\n",
       "         21512, 15358, 22755, 17193, 12678,  3739, 17699, 21266,   433, 10639,\n",
       "          3355,  9726,  5897,  7954, 13608,  2243,  3620, 22570, 14134, 22913,\n",
       "          1849,  8837,  7788, 13391,  3974,  2855, 10189,  8298, 13309,  1197,\n",
       "          4946, 10232,  8860,  4379, 20918, 11002, 22755, 20957, 20113,  7229,\n",
       "         17078,  4443, 21703, 21746, 19626, 20247, 23121,  2115],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(ds[0][\"en\"], ds[0][\"zh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_fn(batch):\n",
    "    # pad the batch\n",
    "    src = [th.tensor(item[\"en\"]) for item in batch]\n",
    "    trg = [th.tensor(item[\"zh\"]) for item in batch]\n",
    "    src = th.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=ds.en_tokenizer.pad_token_id)\n",
    "    trg = th.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=ds.ch_tokenizer.pad_token_id)\n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = th.utils.data.DataLoader(ds, batch_size=16, shuffle=True, collate_fn=collect_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2057,  2064,  2079,  2009,   999,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  2009,  1005,  1055,  2288,  1037, 24650, 31671, 27738,  2075,\n",
       "           1037,  4536,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31409,  1010, 31412,  1010, 31759,  1010, 31728,  1012, 30609,\n",
       "           1024,  2064,  2017,  3191,  2023,  1029,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  1045, 31285,  2514, 14038,  3993,  1010,  2030,  3407,  1010,\n",
       "           2030,  4654, 26415,  2527, 31702,  1010,  2030,  4854,  2043,  1045,\n",
       "           2377,  3056,  4109,  1997,  2189,  1010, 30696,  1045,  1005,  1049,\n",
       "           2025,  9352,  5782,  2017,  2000,  2514,  3599, 31706, 31580,  1041,\n",
       "           2518,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31228,  1010,  2017,  1005,  2222,  2156,  2108, 30805,  2003,\n",
       "          30793,  1040,  2100,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31706, 13032,  1997, 31706,  2715,  3840,  1010, 31706,  2103,\n",
       "           1997,  2414,  3840,  1010,  4654,  2483, 31702,  1999, 31706,  5940,\n",
       "           2301,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  1998,  2746,  2039,  2279,  3204,  2003,  2552,  2226, 30557,\n",
       "           2925, 19960,  1010,  1037,  4013,  2290, 31510,  2000, 31015,  1039,\n",
       "          31559,  1011,  1056,  4542,  1998,  1048, 30886,  7385,  6627,  2053,\n",
       "          21615,  2046,  4200,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31105,  4315, 17417,  9611,  2015,  2966, 15572,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31839,  1010,  2009,  1005,  1055,  2019,  6960,  1012,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  1999,  2755,  1010,  2471, 30886, 29431, 23684,  1999, 31706,\n",
       "           3185,  2003,  2241,  2006,  1037,  2613, 30727,  1010,  1037,  2613,\n",
       "          31451,  2365,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  1998,  2061, 30886,  1050,  2012,  1037, 31842,  2287,  1010,\n",
       "           2008, 31522, 24501,  7856, 31702,  2007,  2033,  1010,  1998, 31706,\n",
       "          31810,  9366,  1997,  4044,  2632,  8347,  1010,  2012,  1037,  2200,\n",
       "           3937,  1048, 30886,  1048,  1010, 10417,  1999,  2007,  2033,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101, 31706,  1061,  1005,  2128,  2552,  2226, 30557,  1040, 30886,\n",
       "           8840,  4691,  1037,  2843,  1997, 31706, 20868,  2671,  1998,  3330,\n",
       "           8882,  2000,  3659,  4235,  2105, 31706, 31829,  1999,  2256,  2291,\n",
       "           1998, 31706,  1061,  1005,  2128, 30525,  1051,  2667,  2000,  1040,\n",
       "          30886,  8840,  2361,  2035,  1997, 31706, 20868,  4007,  5906,  3294,\n",
       "          31414,  1011,  3120,  1012,   102],\n",
       "         [  101,  1045,  2435,  2023,  2831,  2012, 30892,  2025,  2061, 31232,\n",
       "           3283,  2000, 11113, 31422,  2531,  5126,  1010,  1998,  1037,  3232,\n",
       "           2847,  2101,  1010, 31706,  2128,  2001,  1037, 31842, 31826,  2040,\n",
       "           2573, 31706,  2128, 31639, 28642, 31422,  2217,  2026,  2210,  4078,\n",
       "           1047,  1010,  1998,  2016, 14071, 31702,  2000,  2831,  2000,  2033,\n",
       "           1012,   102,     0,     0,     0],\n",
       "         [  101,  2002,  2056,  1010,  1000,  1045,  2228,  2009,  1005,  1055,\n",
       "           2019, 15301,  2000,  2381,  1012,  1000,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  2023,  2003,  1037,  2862,  1997, 31706,  4563,  2136,  1012,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0],\n",
       "         [  101,  1998, 31706,  2128,  2009,  2003,  1999, 31706,  7400,  9563,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0]]),\n",
       " tensor([[ 101, 2769,  812,  ...,    0,    0,    0],\n",
       "         [ 101, 2124, 3022,  ...,    0,    0,    0],\n",
       "         [ 101, 1962, 8024,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101,  800, 6432,  ...,    0,    0,    0],\n",
       "         [ 101, 6821, 3221,  ...,    0,    0,    0],\n",
       "         [ 101, 5659, 1378,  ...,    0,    0,    0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = th.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, total = None, logging_steps=100):\n",
    "    loss_logging = []\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=ds.ch_tokenizer.pad_token_id)\n",
    "    for epoch in trange(epochs):\n",
    "        # for i in tqdm(range(total if total is not None else len(ds)), leave=False):\n",
    "        for i, (src, trg) in tqdm(enumerate(train_loader), total=total if total is not None else len(train_loader), leave=False):\n",
    "            optim.zero_grad()\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0.5)\n",
    "            # out is [batch, len, zh_vocab_size]\n",
    "            # trg is [batch, len]\n",
    "            loss = criterion(out.view(-1, len(ds.ch_tokenizer)), trg.view(-1))\n",
    "            loss_logging.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if i % logging_steps == 0:\n",
    "                print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\")\n",
    "            if total is not None and i >= total:\n",
    "                break\n",
    "    return loss_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7ba1cf7bfd4e26899336b22d804ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ccd539b6704a3d8e100804707a4577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 10.061306953430176\n",
      "Epoch: 0, Step: 10, Loss: 9.845967292785645\n",
      "Epoch: 0, Step: 20, Loss: 6.97701358795166\n",
      "Epoch: 0, Step: 30, Loss: 6.753477573394775\n",
      "Epoch: 0, Step: 40, Loss: 9.146220207214355\n",
      "Epoch: 0, Step: 50, Loss: 6.476015090942383\n",
      "Epoch: 0, Step: 60, Loss: 6.668783664703369\n",
      "Epoch: 0, Step: 70, Loss: 6.552213191986084\n",
      "Epoch: 0, Step: 80, Loss: 6.69061279296875\n",
      "Epoch: 0, Step: 90, Loss: 6.534306049346924\n",
      "Epoch: 0, Step: 100, Loss: 6.1852593421936035\n",
      "Epoch: 0, Step: 110, Loss: 6.4678497314453125\n",
      "Epoch: 0, Step: 120, Loss: 7.004643440246582\n",
      "Epoch: 0, Step: 130, Loss: 6.163119316101074\n",
      "Epoch: 0, Step: 140, Loss: 7.328092575073242\n",
      "Epoch: 0, Step: 150, Loss: 6.864046096801758\n",
      "Epoch: 0, Step: 160, Loss: 6.345973968505859\n",
      "Epoch: 0, Step: 170, Loss: 6.378264427185059\n",
      "Epoch: 0, Step: 180, Loss: 6.9723310470581055\n",
      "Epoch: 0, Step: 190, Loss: 6.064459800720215\n",
      "Epoch: 0, Step: 200, Loss: 6.145749568939209\n",
      "Epoch: 0, Step: 210, Loss: 6.135787487030029\n",
      "Epoch: 0, Step: 220, Loss: 6.153151988983154\n",
      "Epoch: 0, Step: 230, Loss: 6.350038051605225\n",
      "Epoch: 0, Step: 240, Loss: 6.025695323944092\n",
      "Epoch: 0, Step: 250, Loss: 6.220871448516846\n",
      "Epoch: 0, Step: 260, Loss: 5.984378814697266\n",
      "Epoch: 0, Step: 270, Loss: 6.003485202789307\n",
      "Epoch: 0, Step: 280, Loss: 5.800832748413086\n",
      "Epoch: 0, Step: 290, Loss: 6.033677577972412\n",
      "Epoch: 0, Step: 300, Loss: 6.211884021759033\n",
      "Epoch: 0, Step: 310, Loss: 5.67079496383667\n",
      "Epoch: 0, Step: 320, Loss: 5.926328659057617\n",
      "Epoch: 0, Step: 330, Loss: 5.824865341186523\n",
      "Epoch: 0, Step: 340, Loss: 5.846216201782227\n",
      "Epoch: 0, Step: 350, Loss: 5.899908065795898\n",
      "Epoch: 0, Step: 360, Loss: 5.486102104187012\n",
      "Epoch: 0, Step: 370, Loss: 6.0373077392578125\n",
      "Epoch: 0, Step: 380, Loss: 5.79630708694458\n",
      "Epoch: 0, Step: 390, Loss: 6.075170516967773\n",
      "Epoch: 0, Step: 400, Loss: 5.69140100479126\n",
      "Epoch: 0, Step: 410, Loss: 5.701523780822754\n",
      "Epoch: 0, Step: 420, Loss: 5.8508620262146\n",
      "Epoch: 0, Step: 430, Loss: 5.9050164222717285\n",
      "Epoch: 0, Step: 440, Loss: 5.751729965209961\n",
      "Epoch: 0, Step: 450, Loss: 5.985845565795898\n",
      "Epoch: 0, Step: 460, Loss: 5.923342704772949\n",
      "Epoch: 0, Step: 470, Loss: 5.747096061706543\n",
      "Epoch: 0, Step: 480, Loss: 5.78969669342041\n",
      "Epoch: 0, Step: 490, Loss: 5.830208778381348\n",
      "Epoch: 0, Step: 500, Loss: 5.635719299316406\n",
      "Epoch: 0, Step: 510, Loss: 5.690675258636475\n",
      "Epoch: 0, Step: 520, Loss: 6.302981376647949\n",
      "Epoch: 0, Step: 530, Loss: 5.430226802825928\n",
      "Epoch: 0, Step: 540, Loss: 5.672224521636963\n",
      "Epoch: 0, Step: 550, Loss: 5.325995445251465\n",
      "Epoch: 0, Step: 560, Loss: 5.735293388366699\n",
      "Epoch: 0, Step: 570, Loss: 5.885363578796387\n",
      "Epoch: 0, Step: 580, Loss: 5.835219383239746\n",
      "Epoch: 0, Step: 590, Loss: 6.150032997131348\n",
      "Epoch: 0, Step: 600, Loss: 6.1573166847229\n",
      "Epoch: 0, Step: 610, Loss: 5.822987079620361\n",
      "Epoch: 0, Step: 620, Loss: 5.582214832305908\n",
      "Epoch: 0, Step: 630, Loss: 5.628205299377441\n",
      "Epoch: 0, Step: 640, Loss: 5.726812839508057\n",
      "Epoch: 0, Step: 650, Loss: 6.072085380554199\n",
      "Epoch: 0, Step: 660, Loss: 5.730469226837158\n",
      "Epoch: 0, Step: 670, Loss: 5.85931921005249\n",
      "Epoch: 0, Step: 680, Loss: 5.803873538970947\n",
      "Epoch: 0, Step: 690, Loss: 5.697265148162842\n",
      "Epoch: 0, Step: 700, Loss: 5.982180118560791\n",
      "Epoch: 0, Step: 710, Loss: 5.713263034820557\n",
      "Epoch: 0, Step: 720, Loss: 5.343250751495361\n",
      "Epoch: 0, Step: 730, Loss: 6.27084493637085\n",
      "Epoch: 0, Step: 740, Loss: 5.310401916503906\n",
      "Epoch: 0, Step: 750, Loss: 5.738204479217529\n",
      "Epoch: 0, Step: 760, Loss: 6.315343379974365\n",
      "Epoch: 0, Step: 770, Loss: 5.36814022064209\n",
      "Epoch: 0, Step: 780, Loss: 5.947327613830566\n",
      "Epoch: 0, Step: 790, Loss: 5.578488349914551\n",
      "Epoch: 0, Step: 800, Loss: 5.484771251678467\n",
      "Epoch: 0, Step: 810, Loss: 6.370178699493408\n",
      "Epoch: 0, Step: 820, Loss: 5.504859924316406\n",
      "Epoch: 0, Step: 830, Loss: 5.605329990386963\n",
      "Epoch: 0, Step: 840, Loss: 5.564981460571289\n",
      "Epoch: 0, Step: 850, Loss: 6.288450717926025\n",
      "Epoch: 0, Step: 860, Loss: 5.597243309020996\n",
      "Epoch: 0, Step: 870, Loss: 5.662898063659668\n",
      "Epoch: 0, Step: 880, Loss: 5.210269927978516\n",
      "Epoch: 0, Step: 890, Loss: 5.775444030761719\n",
      "Epoch: 0, Step: 900, Loss: 5.825798988342285\n",
      "Epoch: 0, Step: 910, Loss: 5.709985256195068\n",
      "Epoch: 0, Step: 920, Loss: 5.356863021850586\n",
      "Epoch: 0, Step: 930, Loss: 5.627477645874023\n",
      "Epoch: 0, Step: 940, Loss: 5.562939167022705\n",
      "Epoch: 0, Step: 950, Loss: 6.014190196990967\n",
      "Epoch: 0, Step: 960, Loss: 5.584949970245361\n",
      "Epoch: 0, Step: 970, Loss: 5.3876566886901855\n",
      "Epoch: 0, Step: 980, Loss: 5.785289764404297\n",
      "Epoch: 0, Step: 990, Loss: 5.549604892730713\n",
      "Epoch: 0, Step: 1000, Loss: 5.807652473449707\n"
     ]
    }
   ],
   "source": [
    "loss_loggings = train(1, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "trg = [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * len(ds[idx][\"zh\"])\n",
    "txt, l = generate(ds[idx][\"en\"], trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  800,  812,  671,  702,  671,  702, 4638,  671,  702, 8024, 4638,\n",
       "         671,  702, 8024, 3300,  671,  702,  782, 4638, 4638, 8024, 4638, 8024,\n",
       "         800,  812, 4638,  671,  702,  671,  702, 4638, 4638, 8024,  800,  812,\n",
       "        4638,  800,  812, 4638,  800,  812, 4638,  800,  812, 4638,  800,  812,\n",
       "        4638,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 他 们 一 个 一 个 的 一 个 ， 的 一 个 ， 有 一 个 人 的 的 ， 的 ， 他 们 的 一 个 一 个 的 的 ， 他 们 的 他 们 的 他 们 的 他 们 的 他 们 的 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./data/test_en.txt\").read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skip_special(src, trg):\n",
    "    with th.no_grad():\n",
    "        src = th.tensor(src).unsqueeze(0).to(device)\n",
    "        trg = th.tensor(trg).unsqueeze(0).to(device)\n",
    "        out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=-1)\n",
    "    # out is [batch, len, zh_vocab_size]\n",
    "    out = out.squeeze(0)\n",
    "    out = out.argmax(-1)\n",
    "    return ds.ch_tokenizer.decode(out.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6efc22567c493e81a090eb74f951bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"submit.txt\", \"a\") as f:\n",
    "    for line in tqdm(lines):\n",
    "        en = line\n",
    "        zh = generate_skip_special(ds.en_tokenizer.encode(en), [ds.ch_tokenizer.cls_token_id] + [ds.ch_tokenizer.sep_token_id] * 1024)\n",
    "        f.write(f\"{zh}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
