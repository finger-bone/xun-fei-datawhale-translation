{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"readme/","text":"Introduction to Machine Translation This repo talks about, Basic RNN, encoder and decoder model, in note1 Attention mechanism, in note2 Transformer, in note3","title":"Introduction to Machine Translation"},{"location":"readme/#introduction-to-machine-translation","text":"This repo talks about, Basic RNN, encoder and decoder model, in note1 Attention mechanism, in note2 Transformer, in note3","title":"Introduction to Machine Translation"},{"location":"task1/","text":"Introduction to Machine Translation Introduction Machine translation refers to the automatic translation of text from one language to another. It is a subfield of computational linguistics and artificial intelligence. The goal of machine translation is to produce translations that are fluent and accurate, conveying the meaning of the original text. Usually, nowadays, machine translation systems are based on neural networks, which have achieved state-of-the-art performance in many language pairs. These systems are trained on large amounts of parallel text data, which consists of pairs of sentences in two languages. NLP and Machine Translation NLP refers to the field of study that focuses on the interactions between computers and humans through natural language. Machine translation is one of the key applications of NLP, as it involves the automatic translation of text from one language to another. More generally speaking, NLP consists of four major tasks- sequence-to-sequence modeling, text classification, text generation, and text summarization. Machine translation falls under the sequence-to-sequence modeling category, where the goal is to map an input sequence of words in one language to an output sequence of words in another language. Major Concepts in NLP Tokenization Tokenization refers to the process of breaking down text into smaller units, such as words or subwords. This is an essential step in many NLP tasks, including machine translation, as it allows the model to process text at a more granular level. Usually, tokenization involves splitting text on whitespace or punctuation, but more advanced methods, such as subword tokenization, can be used to handle out-of-vocabulary words. Sometimes, there will be special words, aka special tokens, to signal extra information, like start of the sentence, end of the sentence, or padding. A usual way to handle this is to add a special token to the input and output sequences, like <sos> for start of sentence, <eos> for end of sentence, and <pad> for padding. Nowadays, the best library to use for NLP is the transformers library, which is built on top of pytorch. To create a custom tokenizer in the transformers library, we can use the PreTrainedTokenizer class. A simple example of a custom tokenizer is shown below. from transformers import PreTrainedTokenizer class CustomTokenizer(PreTrainedTokenizer): def __init__(self, vocab_file, tokenizer_file): super(CustomTokenizer, self).__init__(vocab_file, tokenizer_file) def _tokenize(self, text): return text.split() def _convert_token_to_id(self, token): return self.vocab[token] def _convert_id_to_token(self, index): return self.ids_to_tokens[index] To add special tokens to pre-trained tokenizer, we can use the add_special_tokens method. If simply expanding the vocab, use add_tokens . A simple example of adding special tokens to a pre-trained tokenizer is shown below. from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') num_added_tokens = tokenizer.add_tokens([\"new_token1\", \"my_new-token2\"]) special_tokens_dict = {\"cls_token\": \"[MY_CLS]\"} num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict) Embedding On a greater level, embedding refers to the process of converting the input to a dense vector representation. This is done by mapping the input to a high-dimensional space, where similar inputs are closer together. Below is an example of training an embedding on the MNIST dataset: import torch as th import torch.nn as nn from torchvision import datasets, transforms # get the minst dataset def get_mnist_data(): # load the data mnist_train = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()) mnist_test = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor()) # create the data loaders train_loader = th.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True) test_loader = th.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False) return train_loader, test_loader class MNISTEmbedding(nn.Module): # To a 2 dimensional space # Use a two-stack of convolutional layers def __init__(self, input_size=28, channels_hidden=32, mlp_hidden = 128): super(MNISTEmbedding, self).__init__() # [btach, 1, x, y] # [batch, 1, x, y] -> [batch, channels_hidden, x, y] self.conv1 = nn.Conv2d(1, channels_hidden, kernel_size=3, stride=1, padding=1) # [batch, channels_hidden, x, y] -> [batch, channels_hidden, x, y] self.conv2 = nn.Conv2d(channels_hidden, channels_hidden, kernel_size=3, stride=1, padding=1) # [batch, channels_hidden, x, y] -> [batch, channels_hidden * x * y] self.flatten = nn.Flatten() # [batch, channels_hidden * x * y] -> [batch, 2] self.mlp = nn.Sequential( nn.Linear(channels_hidden * (input_size ** 2), mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, 2) ) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = self.flatten(x) x = self.mlp(x) return x class MNISTModel(nn.Module): def __init__(self, input_size=28, channels_hidden=32, mlp_hidden=128, embedding_to_result_hidden = 32): super(MNISTModel, self).__init__() self.embedding = MNISTEmbedding(input_size, channels_hidden, mlp_hidden) self.lc_in = nn.Linear(2, embedding_to_result_hidden) self.relu = nn.ReLU() self.lc_out = nn.Linear(embedding_to_result_hidden, 10) def forward(self, x): x = self.embedding(x) x = self.lc_in(x) x = self.relu(x) x = self.lc_out(x) return x # train the model device = \"mps\" train_loader, test_loader = get_mnist_data() model = MNISTModel().to(device=device, dtype=th.float32) optimizer = th.optim.Adam(model.parameters(), lr=1e-4) epochs = 20 logging_steps = 400 from tqdm.notebook import tqdm, trange for epoch in trange(epochs): for i, (x, y) in enumerate(tqdm(train_loader)): x = x.to(device=device, dtype=th.float32) y = y.to(device=device, dtype=th.float32) optimizer.zero_grad() y_pred = model(x) loss = th.nn.functional.cross_entropy(y_pred, y.long()) loss.backward() optimizer.step() if i % logging_steps == 0: print(f\"Epoch {epoch}, step {i}, loss {loss.item()}\") model = model.eval() embedding = model.embedding # Convert test data to embedding vectors embeddings = [] labels = [] for x, y in tqdm(test_loader): x = x.to(device=device, dtype=th.float32) y = y.to(device=device, dtype=th.float32) with th.no_grad(): e = embedding(x) # flatten the batch dimension # detach then extend embeddings.extend(e.detach().cpu().numpy().tolist()) labels.extend(y.detach().cpu().numpy().tolist()) labels = list(map(lambda x: int(x), labels)) import plotly.express as px import pandas as pd # Plot the embeddings with plotly df = pd.DataFrame(embeddings, columns=[\"x\", \"y\"]) df[\"label\"] = list(map(str, labels)) # labels are discrete, so we can use category fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\", opacity=0.7, category_orders={\"label\": [str(i) for i in range(10)]}) # enlarge the size of the graph fig.update_layout(width=800, height=600) fig.show() The embedding can be visualized as shown below: In NLP, embeddings is largely word embeddings, which are dense vector representations of words. These embeddings are trained on large amounts of text data and capture semantic and syntactic information about words. There is a simpler ways to create embedding models with torch.nn.Embedding : # Create an embedding layer with 1000 words and 100 dimensions embedding = nn.Embedding(1000, 100) The input of the embedding layer is the index of the word in the vocabulary, that is, a vector of integers. The output of the embedding layer is a dense vector representation of the word, which can be used as input to a neural network model. Embeddings converts the input to a dense vector representation, which can be used as input to a neural network model. Taking the output of the middle layer of a neural network model, we also get an embedding of the output. Encoder-Decoder Encoder and decoder are two components of a sequence-to-sequence model. The encoder takes an input sequence and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the output sequence. For example, the encoder can be a model that takes an input sentence in English and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the next token in the output sentence. Usually doing so in a loop until decoder generates the end of sentence token. RNN RNNs, or Recurrent Neural Networks, are a type of neural network that is designed to handle sequential data. They are particularly well-suited for tasks such as machine translation, where the input and output sequences are of variable length. A simple RNN model in pytorch is shown below: import torch import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) That is, there are two states in the RNN model, the hidden state and the output state. The hidden state is updated by the input and the previous hidden state, and the output state is updated by the input and the hidden state. During each passing, the hidden state and the output state are updated together. A more commonly used model is the LSTM, long-short term memory model, which is an improved version of the RNN model. The LSTM model has a cell state, which allows it to remember information over long sequences. There is also a RNN model called GRU, gated recurrent unit, which is a simplified version of the LSTM model, also very useful in NLP tasks. LSTM and GRU models are built-in in pytorch. First Solution to Machine Translation The first solution would be a encoder-RNN-decoder model. The encoder takes the input sentence and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the output sentence. A basic encoder-decoder model is implemented under the code, task one folder. Which, doesn't perform well- or any at all, but it is a good starting point to understand the basic concepts of machine translation. Please notice that this is a oversimplified version that doesn't even perform in the task. And no terminology-based method, as the contest requires, is used in this model. Terminologies are only used to expand the vocabulary of the model. This is different from the provided model. Dataloader First, load the data. class MTTrainDataset(Dataset): def __init__(self, train_path, dic_path): self.terms = [ {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1] ] self.data = [ {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(train_path).read().split(\"\\n\")[:-1] ] self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\") self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\") self.en_tokenizer.add_tokens([ term[\"en\"] for term in self.terms ]) self.ch_tokenizer.add_tokens([ term[\"zh\"] for term in self.terms ]) def __len__(self): return len(self.data) def __getitem__(self, index) -> dict: return { \"en\": self.en_tokenizer.encode(self.data[index][\"en\"]), \"zh\": self.ch_tokenizer.encode(self.data[index][\"zh\"]), } def get_raw(self, index): return self.data[index] ds = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\") Encoder Create the encoder, encoder first does embedding, then use RNN to encode the input. # Encoder encodes the input sequence into a sequence of hidden states class Encoder(nn.Module): def __init__(self, en_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1): super(Encoder, self).__init__() self.hidden_dim = hidden_dim # [batch, len] -> [batch, len, embed_dim] self.embed = nn.Embedding(en_vocab_size, embed_dim) # [len, batch, embed_dim] -> [len, batch, hidden_dim], [n_layers == 1, batch, hidden_dim] self.gru = nn.GRU(embed_dim, hidden_dim) self.dropout = nn.Dropout(drop_out_rate) def init_hidden(self, batch_size): # [n_layers == 1, batch, hidden_dim] return th.zeros(1, batch_size, self.hidden_dim).to(device) def forward(self, x): x = self.embed(x) x = self.dropout(x) h = self.init_hidden(x.size(0)) Decoder Then the decoder. Please notice that, the decoder only outputs the next token in the output sequence. In the forward function, x is the input token, the translated token sequence to be in the context, and h is the encoded hidden state of the original input sequence, which contains the information of the input sequence. class Decoder(nn.Module): def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None: super().__init__() # [batch, len == 1] -> [batch, len == 1, embed_dim] self.embed = nn.Embedding(zh_vocab_size, embed_dim) # [batch, len == 1, embed_dim] -> [batch, len == 1, hidden_dim], [n_layers, batch, hidden_dim] self.gru = nn.GRU(embed_dim, hidden_dim) # [batch, hidden_dim] -> [batch, zh_vocab_size] self.fc = nn.Linear(hidden_dim, zh_vocab_size) self.dropout = nn.Dropout(drop_out_rate) def forward(self, x, h): x = self.embed(x) x = self.dropout(x) x = x.permute(1, 0, 2) x, h = self.gru(x, h) x = x.permute(1, 0, 2) x = self.fc(x.squeeze(1)) return x, h Seq2Seq Model Then create the model, which is a combination of the encoder and the decoder. class Seq2Seq(nn.Module): def __init__(self, encoder, decoder): super().__init__() self.encoder = encoder self.decoder = decoder def forward(self, src, trg, src_tokenizer, trg_tokenizer, teacher_forcing_ratio=0.5): # src: [batch, src_len] # trg: [batch, target_len] batch_size = src.size(0) trg_len = trg.size(1) trg_vocab_size = self.decoder.fc.out_features outputs = th.ones(batch_size, trg_len, trg_vocab_size).mul(trg_tokenizer.cls_token_id).to(src.device) # encoder # enc_out: [batch, src_len, hidden_dim], enc_hidden: [n_layers, batch, hidden_dim] enc_out, enc_hidden = self.encoder(src) # decoder # dec_in: [batch, 1] dec_in = trg[:, 0] dec_hidden = enc_hidden for t in range(1, trg_len): dec_out, dec_hidden = self.decoder(dec_in.unsqueeze(1), dec_hidden) # dec_out: [batch, zh_vocab_size] outputs[:, t] = dec_out.squeeze(1) # dec_in: [batch] dec_in = dec_out.argmax(-1) if th.rand(1) < teacher_forcing_ratio: dec_in = trg[:, t] if (dec_in == trg_tokenizer.sep_token_id).all(): if t < trg_len - 1: outputs[:, t+1] = trg_tokenizer.sep_token_id outputs[:, t+2:] = trg_tokenizer.pad_token_id break return outputs Teacher forcing means to use the answer token as the input token in the next time slice when generating the output sequence. This is to help the model to learn the correct translation sequence faster. When doing actual generation, the ratio should be set to zero, so that the model can generate the sequence on its own. This code uses bert tokenizer, so the beginning of sentence is actually cls token, whereas the end of sentence is sep token. Please note that these special tokens have special usage in the bert model, but we only treat them as bos and eos here. Padding Before training, also pad the input to train in batches. def collect_fn(batch): # pad the batch src = [th.tensor(item[\"en\"]) for item in batch] trg = [th.tensor(item[\"zh\"]) for item in batch] src = th.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=ds.en_tokenizer.pad_token_id) trg = th.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=ds.ch_tokenizer.pad_token_id) return src, trg Training Use train the model with the following code, remember to set ignore_index in the loss function to ignore the padding token. def train(epochs, total = None, logging_steps=100): loss_logging = [] criterion = nn.CrossEntropyLoss(ignore_index=ds.ch_tokenizer.pad_token_id) for epoch in trange(epochs): for i, (src, trg) in tqdm(enumerate(train_loader), total=total if total is not None else len(train_loader), leave=False): optim.zero_grad() src = src.to(device) trg = trg.to(device) out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0.5) # out is [batch, len, zh_vocab_size] # trg is [batch, len] loss = criterion(out.view(-1, len(ds.ch_tokenizer)), trg.view(-1)) loss_logging.append(loss.item()) loss.backward() optim.step() if i % logging_steps == 0: print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\") if total is not None and i >= total: break return loss_logging Generating def generate(src, trg): with th.no_grad(): src = th.tensor(src).unsqueeze(0).to(device) trg = th.tensor(trg).unsqueeze(0).to(device) out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0) # out is [batch, len, zh_vocab_size] out = out.squeeze(0) out = out.argmax(-1) return ds.ch_tokenizer.decode(out.tolist()) Results Well the result sucks, but it works. So long as there is a [SEP] in most of the generated result, it is a good sign that the model is learning to generate the sequence, despite its poor performance.","title":"RNN"},{"location":"task1/#introduction-to-machine-translation","text":"","title":"Introduction to Machine Translation"},{"location":"task1/#introduction","text":"Machine translation refers to the automatic translation of text from one language to another. It is a subfield of computational linguistics and artificial intelligence. The goal of machine translation is to produce translations that are fluent and accurate, conveying the meaning of the original text. Usually, nowadays, machine translation systems are based on neural networks, which have achieved state-of-the-art performance in many language pairs. These systems are trained on large amounts of parallel text data, which consists of pairs of sentences in two languages.","title":"Introduction"},{"location":"task1/#nlp-and-machine-translation","text":"NLP refers to the field of study that focuses on the interactions between computers and humans through natural language. Machine translation is one of the key applications of NLP, as it involves the automatic translation of text from one language to another. More generally speaking, NLP consists of four major tasks- sequence-to-sequence modeling, text classification, text generation, and text summarization. Machine translation falls under the sequence-to-sequence modeling category, where the goal is to map an input sequence of words in one language to an output sequence of words in another language.","title":"NLP and Machine Translation"},{"location":"task1/#major-concepts-in-nlp","text":"","title":"Major Concepts in NLP"},{"location":"task1/#tokenization","text":"Tokenization refers to the process of breaking down text into smaller units, such as words or subwords. This is an essential step in many NLP tasks, including machine translation, as it allows the model to process text at a more granular level. Usually, tokenization involves splitting text on whitespace or punctuation, but more advanced methods, such as subword tokenization, can be used to handle out-of-vocabulary words. Sometimes, there will be special words, aka special tokens, to signal extra information, like start of the sentence, end of the sentence, or padding. A usual way to handle this is to add a special token to the input and output sequences, like <sos> for start of sentence, <eos> for end of sentence, and <pad> for padding. Nowadays, the best library to use for NLP is the transformers library, which is built on top of pytorch. To create a custom tokenizer in the transformers library, we can use the PreTrainedTokenizer class. A simple example of a custom tokenizer is shown below. from transformers import PreTrainedTokenizer class CustomTokenizer(PreTrainedTokenizer): def __init__(self, vocab_file, tokenizer_file): super(CustomTokenizer, self).__init__(vocab_file, tokenizer_file) def _tokenize(self, text): return text.split() def _convert_token_to_id(self, token): return self.vocab[token] def _convert_id_to_token(self, index): return self.ids_to_tokens[index] To add special tokens to pre-trained tokenizer, we can use the add_special_tokens method. If simply expanding the vocab, use add_tokens . A simple example of adding special tokens to a pre-trained tokenizer is shown below. from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') num_added_tokens = tokenizer.add_tokens([\"new_token1\", \"my_new-token2\"]) special_tokens_dict = {\"cls_token\": \"[MY_CLS]\"} num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)","title":"Tokenization"},{"location":"task1/#embedding","text":"On a greater level, embedding refers to the process of converting the input to a dense vector representation. This is done by mapping the input to a high-dimensional space, where similar inputs are closer together. Below is an example of training an embedding on the MNIST dataset: import torch as th import torch.nn as nn from torchvision import datasets, transforms # get the minst dataset def get_mnist_data(): # load the data mnist_train = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()) mnist_test = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor()) # create the data loaders train_loader = th.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True) test_loader = th.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False) return train_loader, test_loader class MNISTEmbedding(nn.Module): # To a 2 dimensional space # Use a two-stack of convolutional layers def __init__(self, input_size=28, channels_hidden=32, mlp_hidden = 128): super(MNISTEmbedding, self).__init__() # [btach, 1, x, y] # [batch, 1, x, y] -> [batch, channels_hidden, x, y] self.conv1 = nn.Conv2d(1, channels_hidden, kernel_size=3, stride=1, padding=1) # [batch, channels_hidden, x, y] -> [batch, channels_hidden, x, y] self.conv2 = nn.Conv2d(channels_hidden, channels_hidden, kernel_size=3, stride=1, padding=1) # [batch, channels_hidden, x, y] -> [batch, channels_hidden * x * y] self.flatten = nn.Flatten() # [batch, channels_hidden * x * y] -> [batch, 2] self.mlp = nn.Sequential( nn.Linear(channels_hidden * (input_size ** 2), mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, mlp_hidden), nn.ReLU(), nn.Linear(mlp_hidden, 2) ) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = self.flatten(x) x = self.mlp(x) return x class MNISTModel(nn.Module): def __init__(self, input_size=28, channels_hidden=32, mlp_hidden=128, embedding_to_result_hidden = 32): super(MNISTModel, self).__init__() self.embedding = MNISTEmbedding(input_size, channels_hidden, mlp_hidden) self.lc_in = nn.Linear(2, embedding_to_result_hidden) self.relu = nn.ReLU() self.lc_out = nn.Linear(embedding_to_result_hidden, 10) def forward(self, x): x = self.embedding(x) x = self.lc_in(x) x = self.relu(x) x = self.lc_out(x) return x # train the model device = \"mps\" train_loader, test_loader = get_mnist_data() model = MNISTModel().to(device=device, dtype=th.float32) optimizer = th.optim.Adam(model.parameters(), lr=1e-4) epochs = 20 logging_steps = 400 from tqdm.notebook import tqdm, trange for epoch in trange(epochs): for i, (x, y) in enumerate(tqdm(train_loader)): x = x.to(device=device, dtype=th.float32) y = y.to(device=device, dtype=th.float32) optimizer.zero_grad() y_pred = model(x) loss = th.nn.functional.cross_entropy(y_pred, y.long()) loss.backward() optimizer.step() if i % logging_steps == 0: print(f\"Epoch {epoch}, step {i}, loss {loss.item()}\") model = model.eval() embedding = model.embedding # Convert test data to embedding vectors embeddings = [] labels = [] for x, y in tqdm(test_loader): x = x.to(device=device, dtype=th.float32) y = y.to(device=device, dtype=th.float32) with th.no_grad(): e = embedding(x) # flatten the batch dimension # detach then extend embeddings.extend(e.detach().cpu().numpy().tolist()) labels.extend(y.detach().cpu().numpy().tolist()) labels = list(map(lambda x: int(x), labels)) import plotly.express as px import pandas as pd # Plot the embeddings with plotly df = pd.DataFrame(embeddings, columns=[\"x\", \"y\"]) df[\"label\"] = list(map(str, labels)) # labels are discrete, so we can use category fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\", opacity=0.7, category_orders={\"label\": [str(i) for i in range(10)]}) # enlarge the size of the graph fig.update_layout(width=800, height=600) fig.show() The embedding can be visualized as shown below: In NLP, embeddings is largely word embeddings, which are dense vector representations of words. These embeddings are trained on large amounts of text data and capture semantic and syntactic information about words. There is a simpler ways to create embedding models with torch.nn.Embedding : # Create an embedding layer with 1000 words and 100 dimensions embedding = nn.Embedding(1000, 100) The input of the embedding layer is the index of the word in the vocabulary, that is, a vector of integers. The output of the embedding layer is a dense vector representation of the word, which can be used as input to a neural network model. Embeddings converts the input to a dense vector representation, which can be used as input to a neural network model. Taking the output of the middle layer of a neural network model, we also get an embedding of the output.","title":"Embedding"},{"location":"task1/#encoder-decoder","text":"Encoder and decoder are two components of a sequence-to-sequence model. The encoder takes an input sequence and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the output sequence. For example, the encoder can be a model that takes an input sentence in English and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the next token in the output sentence. Usually doing so in a loop until decoder generates the end of sentence token.","title":"Encoder-Decoder"},{"location":"task1/#rnn","text":"RNNs, or Recurrent Neural Networks, are a type of neural network that is designed to handle sequential data. They are particularly well-suited for tasks such as machine translation, where the input and output sequences are of variable length. A simple RNN model in pytorch is shown below: import torch import torch.nn as nn class RNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(RNN, self).__init__() self.hidden_size = hidden_size self.i2h = nn.Linear(input_size + hidden_size, hidden_size) self.i2o = nn.Linear(input_size + hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, input, hidden): combined = torch.cat((input, hidden), 1) hidden = self.i2h(combined) output = self.i2o(combined) output = self.softmax(output) return output, hidden def initHidden(self): return torch.zeros(1, self.hidden_size) That is, there are two states in the RNN model, the hidden state and the output state. The hidden state is updated by the input and the previous hidden state, and the output state is updated by the input and the hidden state. During each passing, the hidden state and the output state are updated together. A more commonly used model is the LSTM, long-short term memory model, which is an improved version of the RNN model. The LSTM model has a cell state, which allows it to remember information over long sequences. There is also a RNN model called GRU, gated recurrent unit, which is a simplified version of the LSTM model, also very useful in NLP tasks. LSTM and GRU models are built-in in pytorch.","title":"RNN"},{"location":"task1/#first-solution-to-machine-translation","text":"The first solution would be a encoder-RNN-decoder model. The encoder takes the input sentence and encodes it into a fixed-length vector representation, which is then passed to the decoder to generate the output sentence. A basic encoder-decoder model is implemented under the code, task one folder. Which, doesn't perform well- or any at all, but it is a good starting point to understand the basic concepts of machine translation. Please notice that this is a oversimplified version that doesn't even perform in the task. And no terminology-based method, as the contest requires, is used in this model. Terminologies are only used to expand the vocabulary of the model. This is different from the provided model.","title":"First Solution to Machine Translation"},{"location":"task1/#dataloader","text":"First, load the data. class MTTrainDataset(Dataset): def __init__(self, train_path, dic_path): self.terms = [ {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(dic_path).read().split(\"\\n\")[:-1] ] self.data = [ {\"en\": l.split(\"\\t\")[0], \"zh\": l.split(\"\\t\")[1]} for l in open(train_path).read().split(\"\\n\")[:-1] ] self.en_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", cache_dir=\"../../../cache\") self.ch_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\", cache_dir=\"../../../cache\") self.en_tokenizer.add_tokens([ term[\"en\"] for term in self.terms ]) self.ch_tokenizer.add_tokens([ term[\"zh\"] for term in self.terms ]) def __len__(self): return len(self.data) def __getitem__(self, index) -> dict: return { \"en\": self.en_tokenizer.encode(self.data[index][\"en\"]), \"zh\": self.ch_tokenizer.encode(self.data[index][\"zh\"]), } def get_raw(self, index): return self.data[index] ds = MTTrainDataset(\"./data/train.txt\", \"./data/en-zh.dic\")","title":"Dataloader"},{"location":"task1/#encoder","text":"Create the encoder, encoder first does embedding, then use RNN to encode the input. # Encoder encodes the input sequence into a sequence of hidden states class Encoder(nn.Module): def __init__(self, en_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1): super(Encoder, self).__init__() self.hidden_dim = hidden_dim # [batch, len] -> [batch, len, embed_dim] self.embed = nn.Embedding(en_vocab_size, embed_dim) # [len, batch, embed_dim] -> [len, batch, hidden_dim], [n_layers == 1, batch, hidden_dim] self.gru = nn.GRU(embed_dim, hidden_dim) self.dropout = nn.Dropout(drop_out_rate) def init_hidden(self, batch_size): # [n_layers == 1, batch, hidden_dim] return th.zeros(1, batch_size, self.hidden_dim).to(device) def forward(self, x): x = self.embed(x) x = self.dropout(x) h = self.init_hidden(x.size(0))","title":"Encoder"},{"location":"task1/#decoder","text":"Then the decoder. Please notice that, the decoder only outputs the next token in the output sequence. In the forward function, x is the input token, the translated token sequence to be in the context, and h is the encoded hidden state of the original input sequence, which contains the information of the input sequence. class Decoder(nn.Module): def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None: super().__init__() # [batch, len == 1] -> [batch, len == 1, embed_dim] self.embed = nn.Embedding(zh_vocab_size, embed_dim) # [batch, len == 1, embed_dim] -> [batch, len == 1, hidden_dim], [n_layers, batch, hidden_dim] self.gru = nn.GRU(embed_dim, hidden_dim) # [batch, hidden_dim] -> [batch, zh_vocab_size] self.fc = nn.Linear(hidden_dim, zh_vocab_size) self.dropout = nn.Dropout(drop_out_rate) def forward(self, x, h): x = self.embed(x) x = self.dropout(x) x = x.permute(1, 0, 2) x, h = self.gru(x, h) x = x.permute(1, 0, 2) x = self.fc(x.squeeze(1)) return x, h","title":"Decoder"},{"location":"task1/#seq2seq-model","text":"Then create the model, which is a combination of the encoder and the decoder. class Seq2Seq(nn.Module): def __init__(self, encoder, decoder): super().__init__() self.encoder = encoder self.decoder = decoder def forward(self, src, trg, src_tokenizer, trg_tokenizer, teacher_forcing_ratio=0.5): # src: [batch, src_len] # trg: [batch, target_len] batch_size = src.size(0) trg_len = trg.size(1) trg_vocab_size = self.decoder.fc.out_features outputs = th.ones(batch_size, trg_len, trg_vocab_size).mul(trg_tokenizer.cls_token_id).to(src.device) # encoder # enc_out: [batch, src_len, hidden_dim], enc_hidden: [n_layers, batch, hidden_dim] enc_out, enc_hidden = self.encoder(src) # decoder # dec_in: [batch, 1] dec_in = trg[:, 0] dec_hidden = enc_hidden for t in range(1, trg_len): dec_out, dec_hidden = self.decoder(dec_in.unsqueeze(1), dec_hidden) # dec_out: [batch, zh_vocab_size] outputs[:, t] = dec_out.squeeze(1) # dec_in: [batch] dec_in = dec_out.argmax(-1) if th.rand(1) < teacher_forcing_ratio: dec_in = trg[:, t] if (dec_in == trg_tokenizer.sep_token_id).all(): if t < trg_len - 1: outputs[:, t+1] = trg_tokenizer.sep_token_id outputs[:, t+2:] = trg_tokenizer.pad_token_id break return outputs Teacher forcing means to use the answer token as the input token in the next time slice when generating the output sequence. This is to help the model to learn the correct translation sequence faster. When doing actual generation, the ratio should be set to zero, so that the model can generate the sequence on its own. This code uses bert tokenizer, so the beginning of sentence is actually cls token, whereas the end of sentence is sep token. Please note that these special tokens have special usage in the bert model, but we only treat them as bos and eos here.","title":"Seq2Seq Model"},{"location":"task1/#padding","text":"Before training, also pad the input to train in batches. def collect_fn(batch): # pad the batch src = [th.tensor(item[\"en\"]) for item in batch] trg = [th.tensor(item[\"zh\"]) for item in batch] src = th.nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=ds.en_tokenizer.pad_token_id) trg = th.nn.utils.rnn.pad_sequence(trg, batch_first=True, padding_value=ds.ch_tokenizer.pad_token_id) return src, trg","title":"Padding"},{"location":"task1/#training","text":"Use train the model with the following code, remember to set ignore_index in the loss function to ignore the padding token. def train(epochs, total = None, logging_steps=100): loss_logging = [] criterion = nn.CrossEntropyLoss(ignore_index=ds.ch_tokenizer.pad_token_id) for epoch in trange(epochs): for i, (src, trg) in tqdm(enumerate(train_loader), total=total if total is not None else len(train_loader), leave=False): optim.zero_grad() src = src.to(device) trg = trg.to(device) out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0.5) # out is [batch, len, zh_vocab_size] # trg is [batch, len] loss = criterion(out.view(-1, len(ds.ch_tokenizer)), trg.view(-1)) loss_logging.append(loss.item()) loss.backward() optim.step() if i % logging_steps == 0: print(f\"Epoch: {epoch}, Step: {i}, Loss: {loss.item()}\") if total is not None and i >= total: break return loss_logging","title":"Training"},{"location":"task1/#generating","text":"def generate(src, trg): with th.no_grad(): src = th.tensor(src).unsqueeze(0).to(device) trg = th.tensor(trg).unsqueeze(0).to(device) out = model(src, trg, ds.en_tokenizer, ds.ch_tokenizer, teacher_forcing_ratio=0) # out is [batch, len, zh_vocab_size] out = out.squeeze(0) out = out.argmax(-1) return ds.ch_tokenizer.decode(out.tolist())","title":"Generating"},{"location":"task1/#results","text":"Well the result sucks, but it works. So long as there is a [SEP] in most of the generated result, it is a good sign that the model is learning to generate the sequence, despite its poor performance.","title":"Results"},{"location":"task2/","text":"The Night Before the Transformer Introduction The second task aims to improve the performance of the original encoder-rnn-decoder model without introducing the transformer architecture, albeit it has been proven, currently, the best architecture for machine translation tasks. A Recapitulation of the Previous Model In task one, there was a encoder-decoder model with rnn networks. However, it behaved poorly due to, mainly, the context compression. The encoder compresses the context into a fixed-length vector, which is not enough to store all the information. It may work for short sentences, but it fails for longer ones. However, that model got the greater part of the job done. The model was trained on a small dataset, which is not enough to learn the complex patterns of the language. Now, we just need to improve the encoder and decoders. Linear Attention Mechanism Attention mechanism, to put simply, is to weight the importance of different parts of the input sequence. It is a mechanism that allows the model to focus on different parts of the input sequence. An obvious way to improve the encoder-decoder model is to add an extra attention layer in between the encoder and decoder. The attention layer will help the decoder to focus on different parts of the input sequence. An example is as below, class Attention(nn.Module): def __init__(self, hidden_dim, d): super(Attention, self).__init__() self.w = nn.Linear(hidden_dim * 2, d) self.v = nn.Linear(hidden_dim, 1, bias=False) self.activation = nn.Tanh() def forward(self, x, h): h = h.permute(1, 0, 2) # h is [batch, num_layers == 1, hidden_dim] # x is [batch, len, hidden_dim] h = h.expand(-1, x.size(1), -1) # [batch, len, hidden_dim * 2] -> [batch, len, d] w = self.w(th.cat((x, h), dim=-1)) # [batch, len, d] -> [batch, len, 1] -> [batch, len] attn = self.v(w) attn = attn.squeeze(-1) return th.softmax(attn, dim=-1) In the code above, the attn assumes the role of the attention, which is a tensor of shape (batch, len) , where len is the length of the input sequence. So, each element of the tensor refers to the importance of the corresponding part of the input sequence. Using that, we can pay attention to the output vector of the encoder so that we can focus on different parts of the input sequence. class Decoder(nn.Module): def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None: super().__init__() # -> [batch, len] self.attn = Attention(hidden_dim, hidden_dim) # [batch, len == 1] -> [batch, len == 1, embed_dim] self.embed = nn.Embedding(zh_vocab_size, embed_dim) # [len == 1, batch, embed_dim + hidden_dim] -> [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim] self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim) # [batch, hidden_dim * 2 + embed_dim] -> [batch, zh_vocab_size] self.fc = nn.Linear(hidden_dim * 2 + embed_dim, zh_vocab_size) self.dropout = nn.Dropout(drop_out_rate) self.activation = nn.Tanh() def forward(self, x, h, enc_out): # enc_out: [batch, len, hidden_dim] # x is [batch, len == 1] # h is [n_layers == 1, batch, hidden_dim] attn = self.attn(enc_out, h) # [batch, 1, hidden_dim] = [batch, 1, len] * [batch, len, hidden_dim] v = th.bmm(attn.unsqueeze(1), enc_out) # v: [batch, 1, hidden_dim] x = self.embed(x) # x: [batch, len == 1, embed_dim] x = self.dropout(x) rx = th.cat((v, x), dim=-1) rx = self.activation(rx) # rx: [batch, len == 1, embed_dim + hidden_dim] rx = rx.permute(1, 0, 2) out_x, h = self.gru(rx, h) out_x = out_x.permute(1, 0, 2) # out_x: [batch, len == 1, hidden_dim] out_x = out_x.squeeze(1) v = v.squeeze(1) fc_in = th.cat((out_x, v, x.squeeze(1)), dim=-1) out_x = self.fc(fc_in) return out_x, h The result is still horrible. Yet, compared to the previous model, the model can get the length of the sentence almost correct, and it shows some sign of understanding the sentence. After fully training on the provided dataset for one epoch, a much, much better score is achieved. Nevertheless, the model is still scarcely usable. Dot-product Attention Mechanism Query, Key, and Value Dot-product attention is a mechanism that allows the model to focus on different parts of the input sequence. It takes three inputs, query, key, and value, and returns a weighted sum of the values. The query, key, and value are linear transformations of the input sequence. There are three important concepts in the attention mechanism: query, key, and value. Query, the subjective attention vector. It is used to signify how subjectively important part of the value is to the current part of the sequence. By subjectively, it means that the model decides how important the part of the value is to the current part of the sequence, based on its previous knowledge. Key, the objective attention vector. It is used to signify how objectively important part of the value is to the current part of the sequence. By objectively, it means that part of the value is inherently more important to the current part of the sequence. For example, link verbs are less important than nouns. Value, the content vector. It is the part of the sequence that the model should focus on. When the query and key derive from the same input, it is called self attention. When the query and key derive from different inputs, it is called cross attention. Dot-product Attention Math In the attention layer, firstly, a tensor of shape (len, embed) will be transformed into three tensors with the shape (len, d) , where d is the dimension, can be arbitrary, usually equal to the dimension of the embedding vector, so that the input and output have the same shape. Then the math is as follows, \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V where d is the dimension of the key vector. Or for clarity, using the abstract indices, \\text{Attention}(Q_i^j, K_i^j, V_i^j) = \\text{softmax}(\\frac{Q_i^jK_j^k}{\\sqrt{d}})V_k^l Noticing that query and key are actually fungible, our previous definition of query and key is just for the sake of explanation. How it really works is still shrouded in mystery. The divisor \\sqrt{d} is used to prevent the dot product from being too large, which may result in the soft-max function returning 0 or 1. It is purely a practical trick. Implementation A simple implementation of the dor-product attention mechanism is as follows, import torch import torch.nn as nn class SelfAttention(nn.Module): def __init__(self, embed, d): super(SelfAttention, self).__init__() self.Q = nn.Linear(embed, d) self.K = nn.Linear(embed, d) self.V = nn.Linear(embed, d) self.d = d def forward(self, x): # x is [batch, len, embed] # Q, K, V are [batch, len, d] Q = self.Q(x) K = self.K(x) V = self.V(x) # Q, K, V are [batch, len, d] # QK^T is [batch, len, len] # QK^T / sqrt(d) is [batch, len, len] # softmax(QK^T / sqrt(d)) is [batch, len, len] # softmax(QK^T / sqrt(d))V is [batch, len, d] attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5) attn = torch.softmax(attn, dim=-1) out = torch.matmul(attn, V) return out Multi-Head Attention The attention mechanism can be improved by using multiple heads. The multi-head self attention mechanism is a mechanism that allows the model to focus on different parts of the input sequence. To put it more simply, the multi-head mechanism is to use multiple self attention layers in parallel, after splitting the embedding dimension into small chunks, and then concatenate the results. The multi-head mechanism can help the model to focus on different parts of the input sequence. The math is as follows, \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Head}_1, \\text{Head}_2, \\ldots, \\text{Head}_n)W^O where \\text{Head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) is a sub-attention, and W^O is a linear transformation. An simple implementation is as follows, which is a cross attention because the query and key are different. class MultiHeadAttn(nn.Module): def __init__( self, dim: int, heads: int, dropout: float ): super(MultiHeadAttn, self).__init__() self.heads = heads self.dim = dim self.head_dim = dim // heads assert self.head_dim * heads == dim, \"dim must be divisible by heads\" self.q = nn.Linear(dim, dim) self.k = nn.Linear(dim, dim) self.v = nn.Linear(dim, dim) self.fc_out = nn.Linear(dim, dim) self.dropout = nn.Dropout(dropout) def forward(self, x: Tensor, y: Tensor, mask: Tensor | None=None) -> Tensor: batch_size = x.shape[0] # Linear projections Q = self.q(y) K = self.k(x) V = self.v(x) # Reshape for multi-head attention Q = Q.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # Scaled dot-product attention energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5) if mask is not None: mask = mask.unsqueeze(1).unsqueeze(2) # Adjust mask shape for broadcasting energy = energy.masked_fill(mask == 0, float('-inf')) attention = torch.softmax(energy, dim=-1) attention = self.dropout(attention) out = torch.matmul(attention, V) # Concatenate heads out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.dim) # Final linear layer out = self.fc_out(out) return out However, this part will not use the multi-head self attention mechanism. We will use them in the transformer architecture. Other Improvements Without tweaking the model too much, here are some other improvements that can be made to the model. Use LSTM instead of GRU. LSTM is more powerful than GRU, and it can learn more complex patterns in the data. Increase the n_layers of the encoder and decoder. Increasing the number of layers will help the model to learn more complex patterns in the data. Use multi-head self attention in the encoder and decoder. Use more dropout since it is obvious that the model is over-fitting, for continuously generating the same words. Adding extra residual connections in the model. Performing layer normalization after each layer. Increase the size of hidden_dim and d . During the training, there was also a nan loss problem caused by the nan in gradient. When encountering such problem, enforcing gradient clipping or simply decrease the learning rate can solve the problem. Utilizing the previous mentioned techniques, a better result was achieved. There are also other things to note. Make sure to increase dropout to a large value so that the model does not over-fit. If training on a multi-GPU environment, the model must be wrapped in nn.DataParallel to be able to run on multiple GPUs. pytorch has a sort of feature-like bug, that is, when creating multiple tensor with different shapes, it will create cache for each shape, which will consume a lot of memory. To solve this problem, use torch.cuda.empty_cache() to clear the cache, which may work. A better way to get around would be to pad the tensor to the same length or the multiple of the same length. I didn't know that before so it's not in the code, but the memory was substantially higher than it should be, and the training was very slow. enable truncating, or else some very long outliers will cause out of memory error.","title":"Attention"},{"location":"task2/#the-night-before-the-transformer","text":"","title":"The Night Before the Transformer"},{"location":"task2/#introduction","text":"The second task aims to improve the performance of the original encoder-rnn-decoder model without introducing the transformer architecture, albeit it has been proven, currently, the best architecture for machine translation tasks.","title":"Introduction"},{"location":"task2/#a-recapitulation-of-the-previous-model","text":"In task one, there was a encoder-decoder model with rnn networks. However, it behaved poorly due to, mainly, the context compression. The encoder compresses the context into a fixed-length vector, which is not enough to store all the information. It may work for short sentences, but it fails for longer ones. However, that model got the greater part of the job done. The model was trained on a small dataset, which is not enough to learn the complex patterns of the language. Now, we just need to improve the encoder and decoders.","title":"A Recapitulation of the Previous Model"},{"location":"task2/#linear-attention-mechanism","text":"Attention mechanism, to put simply, is to weight the importance of different parts of the input sequence. It is a mechanism that allows the model to focus on different parts of the input sequence. An obvious way to improve the encoder-decoder model is to add an extra attention layer in between the encoder and decoder. The attention layer will help the decoder to focus on different parts of the input sequence. An example is as below, class Attention(nn.Module): def __init__(self, hidden_dim, d): super(Attention, self).__init__() self.w = nn.Linear(hidden_dim * 2, d) self.v = nn.Linear(hidden_dim, 1, bias=False) self.activation = nn.Tanh() def forward(self, x, h): h = h.permute(1, 0, 2) # h is [batch, num_layers == 1, hidden_dim] # x is [batch, len, hidden_dim] h = h.expand(-1, x.size(1), -1) # [batch, len, hidden_dim * 2] -> [batch, len, d] w = self.w(th.cat((x, h), dim=-1)) # [batch, len, d] -> [batch, len, 1] -> [batch, len] attn = self.v(w) attn = attn.squeeze(-1) return th.softmax(attn, dim=-1) In the code above, the attn assumes the role of the attention, which is a tensor of shape (batch, len) , where len is the length of the input sequence. So, each element of the tensor refers to the importance of the corresponding part of the input sequence. Using that, we can pay attention to the output vector of the encoder so that we can focus on different parts of the input sequence. class Decoder(nn.Module): def __init__(self, zh_vocab_size, embed_dim=256, hidden_dim=1024, drop_out_rate=0.1) -> None: super().__init__() # -> [batch, len] self.attn = Attention(hidden_dim, hidden_dim) # [batch, len == 1] -> [batch, len == 1, embed_dim] self.embed = nn.Embedding(zh_vocab_size, embed_dim) # [len == 1, batch, embed_dim + hidden_dim] -> [len == 1, batch, hidden_dim], [n_layers, batch, hidden_dim] self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim) # [batch, hidden_dim * 2 + embed_dim] -> [batch, zh_vocab_size] self.fc = nn.Linear(hidden_dim * 2 + embed_dim, zh_vocab_size) self.dropout = nn.Dropout(drop_out_rate) self.activation = nn.Tanh() def forward(self, x, h, enc_out): # enc_out: [batch, len, hidden_dim] # x is [batch, len == 1] # h is [n_layers == 1, batch, hidden_dim] attn = self.attn(enc_out, h) # [batch, 1, hidden_dim] = [batch, 1, len] * [batch, len, hidden_dim] v = th.bmm(attn.unsqueeze(1), enc_out) # v: [batch, 1, hidden_dim] x = self.embed(x) # x: [batch, len == 1, embed_dim] x = self.dropout(x) rx = th.cat((v, x), dim=-1) rx = self.activation(rx) # rx: [batch, len == 1, embed_dim + hidden_dim] rx = rx.permute(1, 0, 2) out_x, h = self.gru(rx, h) out_x = out_x.permute(1, 0, 2) # out_x: [batch, len == 1, hidden_dim] out_x = out_x.squeeze(1) v = v.squeeze(1) fc_in = th.cat((out_x, v, x.squeeze(1)), dim=-1) out_x = self.fc(fc_in) return out_x, h The result is still horrible. Yet, compared to the previous model, the model can get the length of the sentence almost correct, and it shows some sign of understanding the sentence. After fully training on the provided dataset for one epoch, a much, much better score is achieved. Nevertheless, the model is still scarcely usable.","title":"Linear Attention Mechanism"},{"location":"task2/#dot-product-attention-mechanism","text":"","title":"Dot-product Attention Mechanism"},{"location":"task2/#query-key-and-value","text":"Dot-product attention is a mechanism that allows the model to focus on different parts of the input sequence. It takes three inputs, query, key, and value, and returns a weighted sum of the values. The query, key, and value are linear transformations of the input sequence. There are three important concepts in the attention mechanism: query, key, and value. Query, the subjective attention vector. It is used to signify how subjectively important part of the value is to the current part of the sequence. By subjectively, it means that the model decides how important the part of the value is to the current part of the sequence, based on its previous knowledge. Key, the objective attention vector. It is used to signify how objectively important part of the value is to the current part of the sequence. By objectively, it means that part of the value is inherently more important to the current part of the sequence. For example, link verbs are less important than nouns. Value, the content vector. It is the part of the sequence that the model should focus on. When the query and key derive from the same input, it is called self attention. When the query and key derive from different inputs, it is called cross attention.","title":"Query, Key, and Value"},{"location":"task2/#dot-product-attention-math","text":"In the attention layer, firstly, a tensor of shape (len, embed) will be transformed into three tensors with the shape (len, d) , where d is the dimension, can be arbitrary, usually equal to the dimension of the embedding vector, so that the input and output have the same shape. Then the math is as follows, \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V where d is the dimension of the key vector. Or for clarity, using the abstract indices, \\text{Attention}(Q_i^j, K_i^j, V_i^j) = \\text{softmax}(\\frac{Q_i^jK_j^k}{\\sqrt{d}})V_k^l Noticing that query and key are actually fungible, our previous definition of query and key is just for the sake of explanation. How it really works is still shrouded in mystery. The divisor \\sqrt{d} is used to prevent the dot product from being too large, which may result in the soft-max function returning 0 or 1. It is purely a practical trick.","title":"Dot-product Attention Math"},{"location":"task2/#implementation","text":"A simple implementation of the dor-product attention mechanism is as follows, import torch import torch.nn as nn class SelfAttention(nn.Module): def __init__(self, embed, d): super(SelfAttention, self).__init__() self.Q = nn.Linear(embed, d) self.K = nn.Linear(embed, d) self.V = nn.Linear(embed, d) self.d = d def forward(self, x): # x is [batch, len, embed] # Q, K, V are [batch, len, d] Q = self.Q(x) K = self.K(x) V = self.V(x) # Q, K, V are [batch, len, d] # QK^T is [batch, len, len] # QK^T / sqrt(d) is [batch, len, len] # softmax(QK^T / sqrt(d)) is [batch, len, len] # softmax(QK^T / sqrt(d))V is [batch, len, d] attn = torch.matmul(Q, K.transpose(-2, -1)) / (self.d ** 0.5) attn = torch.softmax(attn, dim=-1) out = torch.matmul(attn, V) return out","title":"Implementation"},{"location":"task2/#multi-head-attention","text":"The attention mechanism can be improved by using multiple heads. The multi-head self attention mechanism is a mechanism that allows the model to focus on different parts of the input sequence. To put it more simply, the multi-head mechanism is to use multiple self attention layers in parallel, after splitting the embedding dimension into small chunks, and then concatenate the results. The multi-head mechanism can help the model to focus on different parts of the input sequence. The math is as follows, \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Head}_1, \\text{Head}_2, \\ldots, \\text{Head}_n)W^O where \\text{Head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) is a sub-attention, and W^O is a linear transformation. An simple implementation is as follows, which is a cross attention because the query and key are different. class MultiHeadAttn(nn.Module): def __init__( self, dim: int, heads: int, dropout: float ): super(MultiHeadAttn, self).__init__() self.heads = heads self.dim = dim self.head_dim = dim // heads assert self.head_dim * heads == dim, \"dim must be divisible by heads\" self.q = nn.Linear(dim, dim) self.k = nn.Linear(dim, dim) self.v = nn.Linear(dim, dim) self.fc_out = nn.Linear(dim, dim) self.dropout = nn.Dropout(dropout) def forward(self, x: Tensor, y: Tensor, mask: Tensor | None=None) -> Tensor: batch_size = x.shape[0] # Linear projections Q = self.q(y) K = self.k(x) V = self.v(x) # Reshape for multi-head attention Q = Q.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) K = K.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) V = V.view(batch_size, -1, self.heads, self.head_dim).transpose(1, 2) # Scaled dot-product attention energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5) if mask is not None: mask = mask.unsqueeze(1).unsqueeze(2) # Adjust mask shape for broadcasting energy = energy.masked_fill(mask == 0, float('-inf')) attention = torch.softmax(energy, dim=-1) attention = self.dropout(attention) out = torch.matmul(attention, V) # Concatenate heads out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.dim) # Final linear layer out = self.fc_out(out) return out However, this part will not use the multi-head self attention mechanism. We will use them in the transformer architecture.","title":"Multi-Head Attention"},{"location":"task2/#other-improvements","text":"Without tweaking the model too much, here are some other improvements that can be made to the model. Use LSTM instead of GRU. LSTM is more powerful than GRU, and it can learn more complex patterns in the data. Increase the n_layers of the encoder and decoder. Increasing the number of layers will help the model to learn more complex patterns in the data. Use multi-head self attention in the encoder and decoder. Use more dropout since it is obvious that the model is over-fitting, for continuously generating the same words. Adding extra residual connections in the model. Performing layer normalization after each layer. Increase the size of hidden_dim and d . During the training, there was also a nan loss problem caused by the nan in gradient. When encountering such problem, enforcing gradient clipping or simply decrease the learning rate can solve the problem. Utilizing the previous mentioned techniques, a better result was achieved. There are also other things to note. Make sure to increase dropout to a large value so that the model does not over-fit. If training on a multi-GPU environment, the model must be wrapped in nn.DataParallel to be able to run on multiple GPUs. pytorch has a sort of feature-like bug, that is, when creating multiple tensor with different shapes, it will create cache for each shape, which will consume a lot of memory. To solve this problem, use torch.cuda.empty_cache() to clear the cache, which may work. A better way to get around would be to pad the tensor to the same length or the multiple of the same length. I didn't know that before so it's not in the code, but the memory was substantially higher than it should be, and the training was very slow. enable truncating, or else some very long outliers will cause out of memory error.","title":"Other Improvements"},{"location":"task3/","text":"Dawn of the Transformer Introduction The previous part of the series presents traditional NLP models. This part will concern itself with the best NLP model up to date, completely new architecture that will be built from scratch. This notebook will introduce the transformer architecture in the order of input to output. Transformer architecture is also a multi-encoder-multi-decoder architecture. Some models also only contains the encoder part, which is used for tasks like text classification, whereas some models only contain the decoder part, which is used for tasks like text generation. For machine translation, both encoder and decoder should be used since the text generation and text understanding are both required. The Transformer Architecture for Machine Translation General Structure We firstly must clearly state the input and output of the transformer architecture in the case of machine translation. The input of the encoder the transformer architecture is a sequence of tokens, of shape [batch_size, src_len] , whilst the decoder accepts an input of shape [batch_size, trg_len] . The transformer model will output a sequence of tokens, of shape [batch_size, trg_len, trg_vocab] , which can later be arg-maxed into [batch_size, trg_len] , with each token id in the second dimension being the next token of that position in the decoder input sequence. For example, if the input of the decoder is <bos>, Attention, is, all, you , with <bos> as the beginning of the sentence, the output of the transformer model will be Attention, is, all, you, need . Then, this part introduces the transformer structure by breaking it down into several parts. The general structure is as follows, Which can be expressed as, enc_out = enc_in |> input_block |> [multi_head_self_attention |> add_and_norm |> feed_forward |> add_and_norm] * N dec_out = [(dec_out |> multi_head_self_attention |> add_and_norm |> feed_forward |> add_and_norm, enc_out) |> multi_head_cross_attention |> add_and_norm |> feed_forward |> add_and_norm] * N |> un_embedding_block where N is the number of layers in the transformer architecture, |> is pipe, and [...] is the list of functions that are applied in order. Input Block Embedding and tokenization have already been introduced in previous parts. Except for the normal tokenization and embedding, another important part of the input is the positional encoding in the transformer architecture. The necessity of positional encoding is justified by the fact that the transformer architecture does not have any recurrence or convolution, in other words, it doesn't process the input token-by-token, and thus it fails to capture the position of the tokens in the input sequence. To deal with the problem, instead of sending in only the embeddings of the tokens, the positional encoding is added to the embeddings. The positional encoding is a vector that is added to the embeddings of the tokens, and it is calculated by the following formula, PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}}) PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}}) where pos is the position of the token in the input sequence, i is the index of the dimension of the positional encoding, and d_{model} is the dimension of the model, which equals the dimension of the embeddings. This equal may seem arbitrary, but it is chosen to make the positional encoding have a smooth curve, which means that the positional encoding will have a similar value for similar positions. In addition, the positional coding using the sine and cosine functions is chosen because the model can learn to attend to relative positions, since the sine of the sum of two angles can be expressed as a function of the sines and cosines of the angles, and so is the cosine. Multi-head Attention Cross Attention If an attention layer requires to pay attention to a sequence based on another sequence, it is called cross attention. For example, the decoder in the machine translation task should pay attention to the encoder output in order to process the output of previous decoder layers. The cross attention is calculated by the following formula, Q = YW^Q \\\\ K = XW^K \\\\ V = XW^V \\\\ So the cross attention can be calculated the same as the self-attention, but the queries are based on the desired output shape. The Mask Technique There is another trick that improves the self-attention mechanism. The mask is used to prevent the model from attending to the future tokens in the input sequence. The mask is a matrix that is added to the attention scores, and it is calculated by the following formula, \\text{mask}_{ij} = \\begin{cases} -\\infty & \\text{if } j > i \\\\ 0 & \\text{otherwise} \\end{cases} where i is the row index and j is the column index of the matrix. So the mask is added to the attention scores before the soft-max function is applied to the attention scores, and the model won't attend to the future tokens in the input sequence. Furthermore, for special tokens like the padding token, the mask is also used to prevent the model from attending to the padding tokens, which can be done by setting the mask value to -\\infty for the padding tokens. So to conclude, the mask should be, \\text{mask}_{ij} = \\begin{cases} -\\infty & \\text{if } j > i \\text{ or } \\text{input}[i] \\text{ is } \\text{padding token} \\\\ 0 & \\text{otherwise} \\end{cases} And mask should be applied to the attention scores before the soft-max function is applied to the attention scores. Add and Norm The add and norm operation is a layer that is added after every sub-layer in the transformer architecture. The add and norm operation is defined as, \\text{AddAndLayerNorm}(x)=\\text{LayerNorm}(x + \\text{SubLayer}(x)) where x is the input to the sub-layer, and \\text{SubLayer}(x) is the output of the sub-layer. This operation is used to prevent the model from exploding or vanishing gradients, and it is also used to stabilize the training process. Residual connections is beneficial for gradient flow because it allows the gradients to flow through the network without vanishing or exploding. This step will be applied after every layer in the transformer architecture. So it will not be repeated in the following sections. Feed Forward The feed forward layer is a simple layer that is used to transform the input to a higher dimension. The feed forward layer is defined as, \\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2 where x is the input to the feed forward layer, W_1 and W_2 are the weights of the feed forward layer, and b_1 and b_2 are the biases of the feed forward layer. The layer is basically just a traditional multi-layer linear neural network with a ReLU activation function. Un-Embedding Block Un-embedding block is the same as previous parts. It just converts from embedding back to vocabulary vector, and if needed, further into token ids. Implementation Positional Encoding class PositionalEncoding(nn.Module): def __init__(self, embedding_dim, max_len=512): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_len, embedding_dim) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, embedding_dim] self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return x # [seq_len, batch_size, embedding_dim] Input Block class PositionalEncoding(nn.Module): def __init__(self, embedding_dim: int, max_len: int=1024): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_len, embedding_dim) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, embedding_dim] self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return x # [seq_len, batch_size, embedding_dim] Add and Norm class AddAndNorm(nn.Module): def __init__(self, embed_d, dropout=0.1): super(AddAndNorm, self).__init__() self.norm = nn.LayerNorm(embed_d) self.dropout = nn.Dropout(dropout) def forward(self, x, y): return self.norm(x + self.dropout(y)) Attention class MultiHeadAttn(nn.Module): def __init__(self, d_model, num_heads, dropout=0.1): super(MultiHeadAttn, self).__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.scale = 1 / np.sqrt(self.d_k) self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def scaled_dot_product_attention(self, q, k, v, mask=None): attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9) attn_probs = torch.softmax(attn_scores, dim=-1) output = torch.matmul(attn_probs, v) return output def split_heads(self, x): batch_size, seq_length, d_model = x.size() return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) def combine_heads(self, x): batch_size, _, seq_length, d_k = x.size() return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) def forward(self, x, y, mask=None): Q = self.split_heads(self.W_q(x)) K = self.split_heads(self.W_k(y)) V = self.split_heads(self.W_v(y)) attn_output = self.scaled_dot_product_attention(Q, K, V, mask) output = self.W_o(self.combine_heads(attn_output)) return self.dropout(output) Feed Forward class FF(nn.Module): def __init__(self, dim: int, hidden_dim: int, dropout: float): super(FF, self).__init__() self.sq = nn.Sequential( nn.Linear(dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, dim) ) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.dropout(self.sq(x)) Encoder Block class EncBlock(nn.Module): def __init__(self, d: int, num_heads: int, hidden_dim: int, dropout: float): super(EncBlock, self).__init__() self.mha = MultiHeadAttn( d, num_heads, dropout) self.ff = FF(d, hidden_dim, dropout) self.add_norm1 = AddAndNorm(d, dropout) self.add_norm2 = AddAndNorm(d, dropout) def forward(self, x, y, mask=None): x = self.add_norm1(x, self.mha(x, y, mask)) return self.add_norm2(x, self.ff(x)) Decoder Block class DecBlock(nn.Module): def __init__(self, d: int=512, num_heads: int=8, hidden_dim: int=1024, dropout: float=0.1): super(DecBlock, self).__init__() self.mha = MultiHeadAttn(d, num_heads, dropout) self.add_and_norm1 = AddAndNorm(d, dropout) self.cross_mha = MultiHeadAttn(d, num_heads, dropout) self.add_and_norm2 = AddAndNorm(d, dropout) self.ff = FF(d, hidden_dim, dropout) self.add_and_norm3 = AddAndNorm(d, dropout) def forward(self, x, y, src_mask=None, trg_mask=None): x = self.add_and_norm1(x, self.mha(x, x, trg_mask)) x = self.add_and_norm2(x, self.cross_mha(x, y, src_mask)) x = self.add_and_norm3(x, self.ff(x)) return x Mask generator def generate_mask(src, tgt): src_mask = (src != 0).unsqueeze(1).unsqueeze(2) tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) seq_length = tgt.size(1) nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool() tgt_mask = tgt_mask & nopeak_mask return src_mask, tgt_mask Transformer class Transformer(nn.Module): def __init__(self, src_vocab: int, tgt_vocab: int, d: int=512, num_heads: int=8, hidden_dim: int=2048, num_enc: int=6, num_dec: int=6, dropout: float=0.1): super(Transformer, self).__init__() self.src_embed = InputBlock(d, src_vocab) self.tgt_embed = InputBlock(d, tgt_vocab) self.encs = nn.ModuleList([ EncBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_enc) ]) self.decs = nn.ModuleList([ DecBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_dec) ]) self.fc = nn.Linear(d, tgt_vocab) def forward(self, src, trg): # src: (batch_size, src_len) # trg: (batch_size, trg_len) src_mask, trg_mask = generate_mask(src, trg) src = self.src_embed(src) trg = self.tgt_embed(trg) for enc in self.encs: src = enc(src, src, src_mask) for dec in self.decs: trg = dec(trg, src, src_mask, trg_mask) return self.fc(trg) Conclusion This part introduces the transformer architecture, which is the best NLP model up to date. The transformer architecture is a multi-encoder-multi-decoder architecture, and it is used for tasks like machine translation, text classification, and text generation. However, the dictionary of the provided data is not used. But we are calling it an end now. The introduction of dictionary can be done by using fine-tuning techniques, manual intervenes, etc.","title":"Transformer"},{"location":"task3/#dawn-of-the-transformer","text":"","title":"Dawn of the Transformer"},{"location":"task3/#introduction","text":"The previous part of the series presents traditional NLP models. This part will concern itself with the best NLP model up to date, completely new architecture that will be built from scratch. This notebook will introduce the transformer architecture in the order of input to output. Transformer architecture is also a multi-encoder-multi-decoder architecture. Some models also only contains the encoder part, which is used for tasks like text classification, whereas some models only contain the decoder part, which is used for tasks like text generation. For machine translation, both encoder and decoder should be used since the text generation and text understanding are both required.","title":"Introduction"},{"location":"task3/#the-transformer-architecture-for-machine-translation","text":"","title":"The Transformer Architecture for Machine Translation"},{"location":"task3/#general-structure","text":"We firstly must clearly state the input and output of the transformer architecture in the case of machine translation. The input of the encoder the transformer architecture is a sequence of tokens, of shape [batch_size, src_len] , whilst the decoder accepts an input of shape [batch_size, trg_len] . The transformer model will output a sequence of tokens, of shape [batch_size, trg_len, trg_vocab] , which can later be arg-maxed into [batch_size, trg_len] , with each token id in the second dimension being the next token of that position in the decoder input sequence. For example, if the input of the decoder is <bos>, Attention, is, all, you , with <bos> as the beginning of the sentence, the output of the transformer model will be Attention, is, all, you, need . Then, this part introduces the transformer structure by breaking it down into several parts. The general structure is as follows, Which can be expressed as, enc_out = enc_in |> input_block |> [multi_head_self_attention |> add_and_norm |> feed_forward |> add_and_norm] * N dec_out = [(dec_out |> multi_head_self_attention |> add_and_norm |> feed_forward |> add_and_norm, enc_out) |> multi_head_cross_attention |> add_and_norm |> feed_forward |> add_and_norm] * N |> un_embedding_block where N is the number of layers in the transformer architecture, |> is pipe, and [...] is the list of functions that are applied in order.","title":"General Structure"},{"location":"task3/#input-block","text":"Embedding and tokenization have already been introduced in previous parts. Except for the normal tokenization and embedding, another important part of the input is the positional encoding in the transformer architecture. The necessity of positional encoding is justified by the fact that the transformer architecture does not have any recurrence or convolution, in other words, it doesn't process the input token-by-token, and thus it fails to capture the position of the tokens in the input sequence. To deal with the problem, instead of sending in only the embeddings of the tokens, the positional encoding is added to the embeddings. The positional encoding is a vector that is added to the embeddings of the tokens, and it is calculated by the following formula, PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}}) PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}}) where pos is the position of the token in the input sequence, i is the index of the dimension of the positional encoding, and d_{model} is the dimension of the model, which equals the dimension of the embeddings. This equal may seem arbitrary, but it is chosen to make the positional encoding have a smooth curve, which means that the positional encoding will have a similar value for similar positions. In addition, the positional coding using the sine and cosine functions is chosen because the model can learn to attend to relative positions, since the sine of the sum of two angles can be expressed as a function of the sines and cosines of the angles, and so is the cosine.","title":"Input Block"},{"location":"task3/#multi-head-attention","text":"","title":"Multi-head Attention"},{"location":"task3/#cross-attention","text":"If an attention layer requires to pay attention to a sequence based on another sequence, it is called cross attention. For example, the decoder in the machine translation task should pay attention to the encoder output in order to process the output of previous decoder layers. The cross attention is calculated by the following formula, Q = YW^Q \\\\ K = XW^K \\\\ V = XW^V \\\\ So the cross attention can be calculated the same as the self-attention, but the queries are based on the desired output shape.","title":"Cross Attention"},{"location":"task3/#the-mask-technique","text":"There is another trick that improves the self-attention mechanism. The mask is used to prevent the model from attending to the future tokens in the input sequence. The mask is a matrix that is added to the attention scores, and it is calculated by the following formula, \\text{mask}_{ij} = \\begin{cases} -\\infty & \\text{if } j > i \\\\ 0 & \\text{otherwise} \\end{cases} where i is the row index and j is the column index of the matrix. So the mask is added to the attention scores before the soft-max function is applied to the attention scores, and the model won't attend to the future tokens in the input sequence. Furthermore, for special tokens like the padding token, the mask is also used to prevent the model from attending to the padding tokens, which can be done by setting the mask value to -\\infty for the padding tokens. So to conclude, the mask should be, \\text{mask}_{ij} = \\begin{cases} -\\infty & \\text{if } j > i \\text{ or } \\text{input}[i] \\text{ is } \\text{padding token} \\\\ 0 & \\text{otherwise} \\end{cases} And mask should be applied to the attention scores before the soft-max function is applied to the attention scores.","title":"The Mask Technique"},{"location":"task3/#add-and-norm","text":"The add and norm operation is a layer that is added after every sub-layer in the transformer architecture. The add and norm operation is defined as, \\text{AddAndLayerNorm}(x)=\\text{LayerNorm}(x + \\text{SubLayer}(x)) where x is the input to the sub-layer, and \\text{SubLayer}(x) is the output of the sub-layer. This operation is used to prevent the model from exploding or vanishing gradients, and it is also used to stabilize the training process. Residual connections is beneficial for gradient flow because it allows the gradients to flow through the network without vanishing or exploding. This step will be applied after every layer in the transformer architecture. So it will not be repeated in the following sections.","title":"Add and Norm"},{"location":"task3/#feed-forward","text":"The feed forward layer is a simple layer that is used to transform the input to a higher dimension. The feed forward layer is defined as, \\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2 where x is the input to the feed forward layer, W_1 and W_2 are the weights of the feed forward layer, and b_1 and b_2 are the biases of the feed forward layer. The layer is basically just a traditional multi-layer linear neural network with a ReLU activation function.","title":"Feed Forward"},{"location":"task3/#un-embedding-block","text":"Un-embedding block is the same as previous parts. It just converts from embedding back to vocabulary vector, and if needed, further into token ids.","title":"Un-Embedding Block"},{"location":"task3/#implementation","text":"","title":"Implementation"},{"location":"task3/#positional-encoding","text":"class PositionalEncoding(nn.Module): def __init__(self, embedding_dim, max_len=512): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_len, embedding_dim) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, embedding_dim] self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return x # [seq_len, batch_size, embedding_dim]","title":"Positional Encoding"},{"location":"task3/#input-block_1","text":"class PositionalEncoding(nn.Module): def __init__(self, embedding_dim: int, max_len: int=1024): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_len, embedding_dim) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) # [max_len, 1, embedding_dim] self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return x # [seq_len, batch_size, embedding_dim]","title":"Input Block"},{"location":"task3/#add-and-norm_1","text":"class AddAndNorm(nn.Module): def __init__(self, embed_d, dropout=0.1): super(AddAndNorm, self).__init__() self.norm = nn.LayerNorm(embed_d) self.dropout = nn.Dropout(dropout) def forward(self, x, y): return self.norm(x + self.dropout(y))","title":"Add and Norm"},{"location":"task3/#attention","text":"class MultiHeadAttn(nn.Module): def __init__(self, d_model, num_heads, dropout=0.1): super(MultiHeadAttn, self).__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.scale = 1 / np.sqrt(self.d_k) self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def scaled_dot_product_attention(self, q, k, v, mask=None): attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9) attn_probs = torch.softmax(attn_scores, dim=-1) output = torch.matmul(attn_probs, v) return output def split_heads(self, x): batch_size, seq_length, d_model = x.size() return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) def combine_heads(self, x): batch_size, _, seq_length, d_k = x.size() return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) def forward(self, x, y, mask=None): Q = self.split_heads(self.W_q(x)) K = self.split_heads(self.W_k(y)) V = self.split_heads(self.W_v(y)) attn_output = self.scaled_dot_product_attention(Q, K, V, mask) output = self.W_o(self.combine_heads(attn_output)) return self.dropout(output)","title":"Attention"},{"location":"task3/#feed-forward_1","text":"class FF(nn.Module): def __init__(self, dim: int, hidden_dim: int, dropout: float): super(FF, self).__init__() self.sq = nn.Sequential( nn.Linear(dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, dim) ) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.dropout(self.sq(x))","title":"Feed Forward"},{"location":"task3/#encoder-block","text":"class EncBlock(nn.Module): def __init__(self, d: int, num_heads: int, hidden_dim: int, dropout: float): super(EncBlock, self).__init__() self.mha = MultiHeadAttn( d, num_heads, dropout) self.ff = FF(d, hidden_dim, dropout) self.add_norm1 = AddAndNorm(d, dropout) self.add_norm2 = AddAndNorm(d, dropout) def forward(self, x, y, mask=None): x = self.add_norm1(x, self.mha(x, y, mask)) return self.add_norm2(x, self.ff(x))","title":"Encoder Block"},{"location":"task3/#decoder-block","text":"class DecBlock(nn.Module): def __init__(self, d: int=512, num_heads: int=8, hidden_dim: int=1024, dropout: float=0.1): super(DecBlock, self).__init__() self.mha = MultiHeadAttn(d, num_heads, dropout) self.add_and_norm1 = AddAndNorm(d, dropout) self.cross_mha = MultiHeadAttn(d, num_heads, dropout) self.add_and_norm2 = AddAndNorm(d, dropout) self.ff = FF(d, hidden_dim, dropout) self.add_and_norm3 = AddAndNorm(d, dropout) def forward(self, x, y, src_mask=None, trg_mask=None): x = self.add_and_norm1(x, self.mha(x, x, trg_mask)) x = self.add_and_norm2(x, self.cross_mha(x, y, src_mask)) x = self.add_and_norm3(x, self.ff(x)) return x","title":"Decoder Block"},{"location":"task3/#mask-generator","text":"def generate_mask(src, tgt): src_mask = (src != 0).unsqueeze(1).unsqueeze(2) tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) seq_length = tgt.size(1) nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool() tgt_mask = tgt_mask & nopeak_mask return src_mask, tgt_mask","title":"Mask generator"},{"location":"task3/#transformer","text":"class Transformer(nn.Module): def __init__(self, src_vocab: int, tgt_vocab: int, d: int=512, num_heads: int=8, hidden_dim: int=2048, num_enc: int=6, num_dec: int=6, dropout: float=0.1): super(Transformer, self).__init__() self.src_embed = InputBlock(d, src_vocab) self.tgt_embed = InputBlock(d, tgt_vocab) self.encs = nn.ModuleList([ EncBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_enc) ]) self.decs = nn.ModuleList([ DecBlock(d, num_heads, hidden_dim, dropout) for _ in range(num_dec) ]) self.fc = nn.Linear(d, tgt_vocab) def forward(self, src, trg): # src: (batch_size, src_len) # trg: (batch_size, trg_len) src_mask, trg_mask = generate_mask(src, trg) src = self.src_embed(src) trg = self.tgt_embed(trg) for enc in self.encs: src = enc(src, src, src_mask) for dec in self.decs: trg = dec(trg, src, src_mask, trg_mask) return self.fc(trg)","title":"Transformer"},{"location":"task3/#conclusion","text":"This part introduces the transformer architecture, which is the best NLP model up to date. The transformer architecture is a multi-encoder-multi-decoder architecture, and it is used for tasks like machine translation, text classification, and text generation. However, the dictionary of the provided data is not used. But we are calling it an end now. The introduction of dictionary can be done by using fine-tuning techniques, manual intervenes, etc.","title":"Conclusion"}]}